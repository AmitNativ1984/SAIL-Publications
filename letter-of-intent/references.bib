@incollection{florenceIntegratedPerceptionControl2020,
  title = {Integrated {{Perception}} and {{Control}} at {{High Speed}}: {{Evaluating Collision Avoidance Maneuvers Without Maps}}},
  shorttitle = {Integrated {{Perception}} and {{Control}} at {{High Speed}}},
  booktitle = {Algorithmic {{Foundations}} of {{Robotics XII}}: {{Proceedings}} of the {{Twelfth Workshop}} on the {{Algorithmic Foundations}} of {{Robotics}}},
  author = {Florence, Pete and Carter, John and Tedrake, Russ},
  editor = {Goldberg, Ken and Abbeel, Pieter and Bekris, Kostas and Miller, Lauren},
  year = {2020},
  pages = {304--319},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-43089-4_20},
  urldate = {2025-06-02},
  abstract = {We present a method for robust high-speed quadrotor flight through unknown cluttered environments using integrated perception and control. Motivated by experiments in which the difficulty of accurate state estimation was a primary limitation on speed, our method forgoes maintaining a map in favor of using only instantaneous depth information in the local frame. This provides robustness in the presence of significant state estimate uncertainty. Additionally, we present approximation methods augmented with spatial partitioning data structures that enable low-latency, real-time reactive control. The probabilistic formulation provides a natural way to integrate reactive obstacle avoidance with arbitrary navigation objectives. We validate the method using a simulated quadrotor race through a forest at high speeds in the presence of increasing state estimate noise. We pair our method with a motion primitive library and compare with a global path-generation and pathfollowing approach.},
  isbn = {978-3-030-43089-4},
  langid = {english},
  keywords = {Agile Flight,path planning},
  file = {/home/amit/Zotero/storage/SYDB5R83/Florence et al. - 2020 - Integrated Perception and Control at High Speed Evaluating Collision Avoidance Maneuvers Without Ma.pdf}
}


@misc{zhouRobustEfficientQuadrotor2019,
  title = {Robust and {{Efficient Quadrotor Trajectory Generation}} for {{Fast Autonomous Flight}}},
  author = {Zhou, Boyu and Gao, Fei and Wang, Luqi and Liu, Chuhao and Shen, Shaojie},
  year = {2019},
  month = jul,
  number = {arXiv:1907.01531},
  eprint = {1907.01531},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.01531},
  urldate = {2025-06-02},
  abstract = {In this paper, we propose a robust and efficient quadrotor motion planning system for fast flight in 3-D complex environments. We adopt a kinodynamic path searching method to find a safe, kinodynamic feasible and minimum-time initial trajectory in the discretized control space. We improve the smoothness and clearance of the trajectory by a B-spline optimization, which incorporates gradient information from a Euclidean distance field (EDF) and dynamic constraints efficiently utilizing the convex hull property of B-spline. Finally, by representing the final trajectory as a non-uniform B-spline, an iterative time adjustment method is adopted to guarantee dynamically feasible and non-conservative trajectories. We validate our proposed method in various complex simulational environments. The competence of the method is also validated in challenging real-world tasks. We release our code as an open-source package.},
  archiveprefix = {arXiv},
  keywords = {Agile Flight,Computer Science - Robotics,path planning},
  file = {/home/amit/Zotero/storage/HY2NCZRN/Zhou et al. - 2019 - Robust and Efficient Quadrotor Trajectory Generation for Fast Autonomous Flight.pdf;/home/amit/Zotero/storage/GJ2SC3VJ/1907.html}
}

@article{osswaldSpikingNeuralNetwork2017,
  title = {A Spiking Neural Network Model of {{3D}} Perception for Event-Based Neuromorphic Stereo Vision Systems},
  author = {Osswald, Marc and Ieng, Sio-Hoi and Benosman, Ryad and Indiveri, Giacomo},
  year = {2017},
  month = jan,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {40703},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/srep40703},
  urldate = {2025-06-01},
  abstract = {Stereo vision is an important feature that enables machine vision systems to perceive their environment in 3D. While machine vision has spawned a variety of software algorithms to solve the stereo-correspondence problem, their implementation and integration in small, fast, and efficient hardware vision systems remains a difficult challenge. Recent advances made in neuromorphic engineering offer a possible solution to this problem, with the use of a new class of event-based vision sensors and neural processing devices inspired by the organizing principles of the brain. Here we propose a radically novel model that solves the stereo-correspondence problem with a spiking neural network that can be directly implemented with massively parallel, compact, low-latency and low-power neuromorphic engineering devices. We validate the model with experimental results, highlighting features that are in agreement with both computational neuroscience stereo vision theories and experimental findings. We demonstrate its features with a prototype neuromorphic hardware system and provide testable predictions on the role of spike-based representations and temporal dynamics in biological stereo vision processing systems.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Computational models,Electrical and electronic engineering,Network models,Visual system},
  file = {/home/amit/Zotero/storage/5UNWRT4M/Osswald et al. - 2017 - A spiking neural network model of 3D perception for event-based neuromorphic stereo vision systems.pdf}
}


@article{cuadradoOpticalFlowEstimation2023,
  title = {Optical Flow Estimation from Event-Based Cameras and Spiking Neural Networks},
  author = {Cuadrado, Javier and Ran{\c c}on, Ulysse and Cottereau, Benoit R. and Barranco, Francisco and Masquelier, Timoth{\'e}e},
  year = {2023},
  month = may,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1160034},
  urldate = {2025-06-01},
  abstract = {Event-based cameras are raising interest within the computer vision community. These sensors operate with asynchronous pixels, emitting events, or ``spikes'', when the luminance change at a given pixel since the last event surpasses a certain threshold. Thanks to their inherent qualities, such as their low power consumption, low latency and high dynamic range, they seem particularly tailored to applications with challenging temporal constraints and safety requirements. Event-based sensors are an excellent fit for Spiking Neural Networks (SNNs), since the coupling of an asynchronous sensor with neuromorphic hardware can yield real-time systems with minimal power requirements. In this work, we seek to develop one such system, using both event sensor data from the DSEC dataset and spiking neural networks to estimate optical flow for driving scenarios. We propose a U-Net-like SNN which, after supervised training, is able to make dense optical flow estimations. To do so, we encourage both minimal norm for the error vector and minimal angle between ground-truth and predicted flow, training our model with back-propagation using a surrogate gradient. In addition, the use of 3d convolutions allows us to capture the dynamic nature of the data by increasing the temporal receptive fields. Upsampling after each decoding stage ensures that each decoder's output contributes to the final estimation. Thanks to separable convolutions, we have been able to develop a light model (when compared to competitors) that can nonetheless yield reasonably accurate optical flow estimates.},
  langid = {english},
  keywords = {.toread,Edge AI,event vision,neuromorphic computing,Neuromorphic processing,optical flow,spiking neural networks},
  file = {/home/amit/Zotero/storage/5WJWXDJR/Cuadrado et al. - 2023 - Optical flow estimation from event-based cameras and spiking neural networks.pdf}
}


@article{siddiqueLowCostNeuromorphic2023,
  title = {A Low Cost Neuromorphic Learning Engine Based on a High Performance Supervised {{SNN}} Learning Algorithm},
  author = {Siddique, Ali and Vai, Mang I. and Pun, Sio Hang},
  year = {2023},
  month = apr,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {6280},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-32120-7},
  urldate = {2025-06-01},
  abstract = {Spiking neural networks (SNNs) are more energy- and resource-efficient than artificial neural networks (ANNs). However, supervised SNN learning is a challenging task due to non-differentiability of spikes and computation of complex terms. Moreover, the design of SNN learning engines is not an easy task due to limited hardware resources and tight energy constraints. In this article, a novel hardware-efficient SNN back-propagation scheme that offers fast convergence is proposed. The learning scheme does not require any complex operation such as error normalization and weight-threshold balancing, and can achieve an accuracy of around 97.5\% on MNIST dataset using only 158,800 synapses. The multiplier-less inference engine trained using the proposed hard sigmoid SNN training (HaSiST) scheme can operate at a frequency of 135 MHz and consumes only 1.03 slice registers per synapse, 2.8 slice look-up tables, and can infer about 0.03\$\${\textbackslash}times \{{\textbackslash}varvec\{10\}\}{\textasciicircum}\{{\textbackslash}varvec\{9\}\}\$\$features in a second, equivalent to 9.44 giga synaptic operations per second (GSOPS). The article also presents a high-speed, cost-efficient SNN training engine that consumes only 2.63 slice registers per synapse, 37.84 slice look-up tables per synapse, and can operate at a maximum computational frequency of around 50 MHz on a Virtex 6 FPGA.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science,Electrical and electronic engineering,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/R84AHNYV/Siddique et al. - 2023 - A low cost neuromorphic learning engine based on a high performance supervised SNN learning algorith.pdf}
}


@article{farsaLowCostHighSpeedNeuromorphic2019,
  title = {A {{Low-Cost High-Speed Neuromorphic Hardware Based}} on {{Spiking Neural Network}}},
  author = {Farsa, Edris Zaman and Ahmadi, Arash and Maleki, Mohammad Ali and Gholami, Morteza and Rad, Hima Nikafshan},
  year = {2019},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  volume = {66},
  number = {9},
  pages = {1582--1586},
  issn = {1558-3791},
  doi = {10.1109/TCSII.2019.2890846},
  urldate = {2025-06-01},
  abstract = {Neuromorphic is a relatively new interdisciplinary research topic, which employs various fields of science and technology, such as electronic, computer, and biology. Neuromorphic systems consist of software/hardware systems, which are utilized to implement the neural networks based on human brain functionalities. The goal of neuromorphic systems is to mimic the biologically inspired concepts of the nervous systems, envisioned to provide advantages, such as lower power consumption, fault tolerance, and massive parallelism for the next generation of computers. This brief presents a neural computing hardware unit and a neuromorphic system architecture based on a modified leaky integrate and fire neuron model in a spiking neural network for a pattern recognition task in register-transfer level. The neuron model and the spiking network are explored, considering digital implementation, targeting low-cost high-speed large-scale systems. Results of the hardware synthesis and implementation on field-programmable gate array are presented as a proof of concept. Accordingly, the maximum frequency of the implemented neuron model and spiking network are 412.371 MHz and 189.071 MHz, respectively.},
  keywords = {Biological neural networks,Biological system modeling,Computational modeling,field-programmable gate array (FPGA),Hardware,hardware implementation,leaky integrate and fire (LIF) model,neural computing hardware unit (NCHU),neuromorphic,Neuromorphic processing,Neuromorphics,Neurons,Pattern recognition,spiking neural network (SNN)},
  file = {/home/amit/Zotero/storage/FB6X5HER/8600358.html}
}


@inproceedings{LearningAgileVisionBasedDroneFlightFromSimulationtoReality,
  title = {Learning Agile, Vision-Based Drone Flight: {{From}} Simulation to Reality},
  booktitle = {Robotics Research},
  author = {Scaramuzza, Davide and Kaufmann, Elia},
  editor = {Billard, Aude and Asfour, Tamim and Khatib, Oussama},
  year = {2023},
  pages = {11--18},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  abstract = {We present our latest research in learning deep sensorimotor policies for agile, vision-based quadrotor flight. We show methodologies for the successful transfer of such policies from simulation to the real world. In addition, we discuss the open research questions that still need to be answered to improve the agility and robustness of autonomous drones toward human-pilot performance.},
  isbn = {978-3-031-25555-7}
}

@misc{DenseContinuousTimeOptical,
  title = {Dense {{Continuous-Time Optical Flow From Event Cameras}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-05-30},
  howpublished = {https://ieeexplore.ieee.org/document/10419040},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/LVL5ZK7E/10419040.html}
}

@article{falangaDynamicObstacleAvoidance2020,
  title = {Dynamic Obstacle Avoidance for Quadrotors with Event Cameras},
  author = {Falanga, Davide and Kleber, Kevin and Scaramuzza, Davide},
  year = {2020},
  month = mar,
  journal = {Science Robotics},
  volume = {5},
  number = {40},
  pages = {eaaz9712},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scirobotics.aaz9712},
  urldate = {2025-05-30},
  abstract = {Today's autonomous drones have reaction times of tens of milliseconds, which is not enough for navigating fast in complex dynamic environments. To safely avoid fast moving objects, drones need low-latency sensors and algorithms. We departed from state-of-the-art approaches by using event cameras, which are bioinspired sensors with reaction times of microseconds. Our approach exploits the temporal information contained in the event stream to distinguish between static and dynamic objects and leverages a fast strategy to generate the motor commands necessary to avoid the approaching obstacles. Standard vision algorithms cannot be applied to event cameras because the output of these sensors is not images but a stream of asynchronous events that encode per-pixel intensity changes. Our resulting algorithm has an overall latency of only 3.5 milliseconds, which is sufficient for reliable detection and avoidance of fast-moving obstacles. We demonstrate the effectiveness of our approach on an autonomous quadrotor using only onboard sensing and computation. Our drone was capable of avoiding multiple obstacles of different sizes and shapes, at relative speeds up to 10 meters/second, both indoors and outdoors.},
  keywords = {.toread,Agile Flight,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/NPN24XVJ/Falanga et al. - 2020 - Dynamic obstacle avoidance for quadrotors with event cameras.pdf}
}

@article{falangaHowFastToo2019,
  title = {How {{Fast Is Too Fast}}? {{The Role}} of {{Perception Latency}} in {{High-Speed Sense}} and {{Avoid}}},
  shorttitle = {How {{Fast Is Too Fast}}?},
  author = {Falanga, Davide and Kim, Suseong and Scaramuzza, Davide},
  year = {2019},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {2},
  pages = {1884--1891},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2019.2898117},
  urldate = {2025-05-30},
  abstract = {In this work, we study the effects that perception latency has on the maximum speed a robot can reach to safely navigate through an unknown cluttered environment. We provide a general analysis that can serve as a baseline for future quantitative reasoning for design trade-offs in autonomous robot navigation. We consider the case where the robot is modeled as a linear second-order system with bounded input and navigates through static obstacles. Also, we focus on a scenario where the robot wants to reach a target destination in as little time as possible, and therefore cannot change its longitudinal velocity to avoid obstacles. We show how the maximum latency that the robot can tolerate to guarantee safety is related to the desired speed, the range of its sensing pipeline, and the actuation limitations of the platform (i.e., the maximum acceleration it can produce). As a particular case study, we compare monocular and stereo frame-based cameras against novel, low-latency sensors, such as event cameras, in the case of quadrotor flight. To validate our analysis, we conduct experiments on a quadrotor platform equipped with an event camera to detect and avoid obstacles thrown towards the robot. To the best of our knowledge, this is the first theoretical work in which perception and actuation limitations are jointly considered to study the performance of a robotic platform in high-speed navigation.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Agile Flight,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/LGMK7RMT/Falanga et al. - 2019 - How Fast Is Too Fast The Role of Perception Latency in High-Speed Sense and Avoid.pdf}
}

@misc{forraiEventbasedAgileObject2023,
  title = {Event-Based {{Agile Object Catching}} with a {{Quadrupedal Robot}}},
  author = {Forrai, Benedek and Miki, Takahiro and Gehrig, Daniel and Hutter, Marco and Scaramuzza, Davide},
  year = {2023},
  month = apr,
  number = {arXiv:2303.17479},
  eprint = {2303.17479},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.17479},
  urldate = {2025-05-30},
  abstract = {Quadrupedal robots are conquering various indoor and outdoor applications due to their ability to navigate challenging uneven terrains. Exteroceptive information greatly enhances this capability since perceiving their surroundings allows them to adapt their controller and thus achieve higher levels of robustness. However, sensors such as LiDARs and RGB cameras do not provide sufficient information to quickly and precisely react in a highly dynamic environment since they suffer from a bandwidth-latency tradeoff. They require significant bandwidth at high frame rates while featuring significant perceptual latency at lower frame rates, thereby limiting their versatility on resource-constrained platforms. In this work, we tackle this problem by equipping our quadruped with an event camera, which does not suffer from this tradeoff due to its asynchronous and sparse operation. In leveraging the low latency of the events, we push the limits of quadruped agility and demonstrate high-speed ball catching for the first time. We show that our quadruped equipped with an event camera can catch objects with speeds up to 15 m/s from 4 meters, with a success rate of 83\%. Using a VGA event camera, our method runs at 100 Hz on an NVIDIA Jetson Orin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/XVZTY3CN/Forrai et al. - 2023 - Event-based Agile Object Catching with a Quadrupedal Robot.pdf;/home/amit/Zotero/storage/E43CQILQ/2303.html}
}

@inproceedings{gehrigERAFTDenseOptical2021,
  title = {E-{{RAFT}}: {{Dense Optical Flow}} from {{Event Cameras}}},
  shorttitle = {E-{{RAFT}}},
  booktitle = {2021 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Gehrig, Mathias and Millhausler, Mario and Gehrig, Daniel and Scaramuzza, Davide},
  year = {2021},
  month = dec,
  pages = {197--206},
  publisher = {IEEE},
  address = {London, United Kingdom},
  doi = {10.1109/3DV53792.2021.00030},
  urldate = {2025-05-30},
  abstract = {We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23\% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66\%.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-2688-6},
  langid = {english},
  keywords = {.toread,Agile Flight,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/6VEDSLZ2/Gehrig et al. - 2021 - E-RAFT Dense Optical Flow from Event Cameras.pdf}
}

@misc{gehrigERAFTDenseOptical2021a,
  title = {E-{{RAFT}}: {{Dense Optical Flow}} from {{Event Cameras}}},
  shorttitle = {E-{{RAFT}}},
  author = {Gehrig, Mathias and Millh{\"a}usler, Mario and Gehrig, Daniel and Scaramuzza, Davide},
  year = {2021},
  month = oct,
  number = {arXiv:2108.10552},
  eprint = {2108.10552},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.10552},
  urldate = {2025-05-30},
  abstract = {We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23\% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with a maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66\%.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Computer Vision and Pattern Recognition,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/F6AQMRXB/Gehrig et al. - 2021 - E-RAFT Dense Optical Flow from Event Cameras.pdf;/home/amit/Zotero/storage/DFUBUMAK/2108.html}
}

@article{gehrigLowlatencyAutomotiveVision2024,
  title = {Low-Latency Automotive Vision with Event Cameras},
  author = {Gehrig, Daniel and Scaramuzza, Davide},
  year = {2024},
  month = may,
  journal = {Nature},
  volume = {629},
  number = {8014},
  pages = {1034--1040},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07409-w},
  abstract = {The computer vision algorithms used currently in advanced driver assistance systems rely on image-based RGB cameras, leading to a critical bandwidth--latency trade-off for delivering safe driving experiences. To address this, event cameras have emerged as alternative vision sensors. Event cameras measure the changes in intensity asynchronously, offering high temporal resolution and sparsity, markedly reducing bandwidth and latency requirements1. Despite these advantages, event-camera-based algorithms are either highly efficient but lag behind image-based ones in terms of accuracy or sacrifice the sparsity and efficiency of events to achieve comparable results. To overcome this, here we propose a hybrid event- and frame-based object detector that preserves the advantages of each modality and thus does not suffer from this trade-off. Our method exploits the high temporal resolution and sparsity of events and the rich but low temporal resolution information in standard images to generate efficient, high-rate object detections, reducing perceptual and computational latency. We show that the use of a 20~frames~per second (fps) RGB camera plus an event camera can achieve the same latency as a 5,000-fps camera with the bandwidth of a 45-fps camera without compromising accuracy. Our approach paves the way for efficient and robust perception in edge-case scenarios by uncovering the potential of event cameras2.},
  keywords = {Computer science,Engineering,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/J52NMI4U/Gehrig and Scaramuzza - 2024 - Low-latency automotive vision with event cameras.pdf}
}


@article{gehrigLowlatencyAutomotiveVision2024a,
  title = {Low-Latency Automotive Vision with Event Cameras},
  author = {Gehrig, Daniel and Scaramuzza, Davide},
  year = {2024},
  month = may,
  journal = {Nature},
  volume = {629},
  number = {8014},
  pages = {1034--1040},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07409-w},
  urldate = {2025-05-30},
  abstract = {The computer vision algorithms used currently in advanced driver assistance systems rely on image-based RGB cameras, leading to a critical bandwidth--latency trade-off for delivering safe driving experiences. To address this, event cameras have emerged as alternative vision sensors. Event cameras measure the changes in intensity asynchronously, offering high temporal resolution and sparsity, markedly reducing bandwidth and latency requirements1. Despite these advantages, event-camera-based algorithms are either highly efficient but lag behind image-based ones in terms of accuracy or sacrifice the sparsity and efficiency of events to achieve comparable results. To overcome this, here we propose a hybrid event- and frame-based object detector that preserves the advantages of each modality and thus does not suffer from this trade-off. Our method exploits the high temporal resolution and sparsity of events and the rich but low temporal resolution information in standard images to generate efficient, high-rate object detections, reducing perceptual and computational latency. We show that the use of a 20~frames~per second (fps) RGB camera plus an event camera can achieve the same latency as a 5,000-fps camera with the bandwidth of a 45-fps camera without compromising accuracy. Our approach paves the way for efficient and robust perception in edge-case scenarios by uncovering the potential of event cameras2.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Engineering,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/J52NMI4U/Gehrig and Scaramuzza - 2024 - Low-latency automotive vision with event cameras.pdf}
}

@misc{iaboniEventbasedSpikingNeural2024,
  title = {Event-Based {{Spiking Neural Networks}} for {{Object Detection}}: {{A Review}} of {{Datasets}}, {{Architectures}}, {{Learning Rules}}, and {{Implementation}}},
  shorttitle = {Event-Based {{Spiking Neural Networks}} for {{Object Detection}}},
  author = {Iaboni, Craig and Abichandani, Pramod},
  year = {2024},
  month = nov,
  number = {arXiv:2411.17006},
  eprint = {2411.17006},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.17006},
  urldate = {2025-05-30},
  abstract = {Spiking Neural Networks (SNNs) represent a biologically inspired paradigm offering an energy-efficient alternative to conventional artificial neural networks (ANNs) for Computer Vision (CV) applications. This paper presents a systematic review of datasets, architectures, learning methods, implementation techniques, and evaluation methodologies used in CV-based object detection tasks using SNNs. Based on an analysis of 151 journal and conference articles, the review codifies: 1) the effectiveness of fully connected, convolutional, and recurrent architectures; 2) the performance of direct unsupervised, direct supervised, and indirect learning methods; and 3) the trade-offs in energy consumption, latency, and memory in neuromorphic hardware implementations. An open-source repository along with detailed examples of Python code and resources for building SNN models, event-based data processing, and SNN simulations are provided. Key challenges in SNN training, hardware integration, and future directions for CV applications are also identified.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/6ACK8TL7/Iaboni and Abichandani - 2024 - Event-based Spiking Neural Networks for Object Detection A Review of Datasets, Architectures, Learn.pdf;/home/amit/Zotero/storage/KSTTLRA6/2411.html}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  year = {1997},
  month = dec,
  journal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00011-7},
  urldate = {2025-05-30},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  keywords = {.toread,Computational complexity,Integrate-and-fire neutron,Lower bounds,Neuromorphic processing,Sigmoidal neural nets,Spiking neuron},
  file = {/home/amit/Zotero/storage/V4AJLH7K/S0893608097000117.html}
}

@article{maassNetworksSpikingNeurons1997a,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  year = {1997},
  month = dec,
  journal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {08936080},
  doi = {10.1016/S0893-6080(97)00011-7},
  urldate = {2025-05-30},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. {\copyright} 1997 Elsevier Science Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/JBQH96PH/Maass - 1997 - Networks of spiking neurons The third generation of neural network models.pdf}
}

@article{messikommerDataDrivenFeatureTracking2025,
  title = {Data-{{Driven Feature Tracking}} for {{Event Cameras With}} and {{Without Frames}}},
  author = {Messikommer, Nico and Fang, Carter and Gehrig, Mathias and Cioffi, Giovanni and Scaramuzza, Davide},
  year = {2025},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {47},
  number = {5},
  pages = {3706--3717},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2025.3536016},
  urldate = {2025-05-30},
  abstract = {Because of their high temporal resolution, increased resilience to motion blur, and very sparse output, event cameras have been shown to be ideal for low-latency and low-bandwidth feature tracking, even in challenging scenarios. Existing feature tracking methods for event cameras are either handcrafted or derived from first principles but require extensive parameter tuning, are sensitive to noise, and do not generalize to different scenarios due to unmodeled effects. To tackle these deficiencies, we introduce the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in an intensity frame. We achieve robust performance via a novel frame attention module, which shares information across feature tracks. Our tracker is designed to operate in two distinct configurations: solely with events or in a hybrid mode incorporating both events and frames. The hybrid model offers two setups: an aligned configuration where the event and frame cameras share the same viewpoint, and a hybrid stereo configuration where the event camera and the standard camera are positioned side-by-side. This side-by-side arrangement is particularly valuable as it provides depth information for each feature track, enhancing its utility in applications such as visual odometry and simultaneous localization and mapping.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Cameras,Computer architecture,Estimation,Event cameras,Event detection,Feature extraction,feature tracking,Neuromorphic processing,Optical flow,sparse disparity estimation,Standards,Streams,Three-dimensional displays,Tracking},
  file = {/home/amit/Zotero/storage/DS2JHFJ8/Arxiv24_Messikommer.pdf;/home/amit/Zotero/storage/GGPSUAAK/10857596.html}
}

@inproceedings{rebecqRealtimeVisualInertialOdometry2017,
  title = {Real-Time {{Visual-Inertial Odometry}} for {{Event Cameras}} Using {{Keyframe-based Nonlinear Optimization}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2017},
  author = {Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
  year = {2017},
  pages = {16},
  publisher = {British Machine Vision Association},
  address = {London, UK},
  doi = {10.5244/C.31.16},
  urldate = {2025-05-30},
  abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. We propose a novel, accurate tightly-coupled visual-inertial odometry pipeline for such cameras that leverages their outstanding properties to estimate the camera ego-motion in challenging conditions, such as high-speed motion or high dynamic range scenes. The method tracks a set of features (extracted on the image plane) through time. To achieve that, we consider events in overlapping spatio-temporal windows and align them using the current camera motion and scene structure, yielding motion-compensated event frames. We then combine these feature tracks in a keyframebased, visual-inertial odometry algorithm based on nonlinear optimization to estimate the camera's 6-DOF pose, velocity, and IMU biases. The proposed method is evaluated quantitatively on the public Event Camera Dataset [19] and significantly outperforms the state-of-the-art [28], while being computationally much more efficient: our pipeline can run much faster than real-time on a laptop and even on a smartphone processor. Furthermore, we demonstrate qualitatively the accuracy and robustness of our pipeline on a large-scale dataset, and an extremely high-speed dataset recorded by spinning an event camera on a leash at 850 deg/s.},
  isbn = {978-1-901725-60-5},
  langid = {english},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/Q7WT69BH/Rebecq et al. - 2017 - Real-time Visual-Inertial Odometry for Event Cameras using Keyframe-based Nonlinear Optimization.pdf;/home/amit/Zotero/storage/WC3L7JKL/search.html}
}

@misc{romeroDreamFlyModelBased2025a,
  title = {Dream to {{Fly}}: {{Model-Based Reinforcement Learning}} for {{Vision-Based Drone Flight}}},
  shorttitle = {Dream to {{Fly}}},
  author = {Romero, Angel and Shenai, Ashwin and Geles, Ismail and Aljalbout, Elie and Scaramuzza, Davide},
  year = {2025},
  month = jan,
  number = {arXiv:2501.14377},
  eprint = {2501.14377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14377},
  urldate = {2025-05-30},
  abstract = {Autonomous drone racing has risen as a challenging robotic benchmark for testing the limits of learning, perception, planning, and control. Expert human pilots are able to agilely fly a drone through a race track by mapping the real-time feed from a single onboard camera directly to control commands. Recent works in autonomous drone racing attempting direct pixel-to-commands control policies (without explicit state estimation) have relied on either intermediate representations that simplify the observation space or performed extensive bootstrapping using Imitation Learning (IL). This paper introduces an approach that learns policies from scratch, allowing a quadrotor to autonomously navigate a race track by directly mapping raw onboard camera pixels to control commands, just as human pilots do. By leveraging model-based reinforcement learning{\textasciitilde}(RL) - specifically DreamerV3 - we train visuomotor policies capable of agile flight through a race track using only raw pixel observations. While model-free RL methods such as PPO struggle to learn under these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors. Moreover, because our policies learn directly from pixel inputs, the perception-aware reward term employed in previous RL approaches to guide the training process is no longer needed. Our experiments demonstrate in both simulation and real-world flight how the proposed approach can be deployed on agile quadrotors. This approach advances the frontier of vision-based autonomous flight and shows that model-based RL is a promising direction for real-world robotics.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/GHZRWZJR/Romero et al. - 2025 - Dream to Fly Model-Based Reinforcement Learning for Vision-Based Drone Flight.pdf;/home/amit/Zotero/storage/K52Y2VFV/2501.html}
}

@misc{shariffEventbasedYOLOObject2023,
  title = {Event-Based {{YOLO Object Detection}}: {{Proof}} of {{Concept}} for {{Forward Perception System}}},
  shorttitle = {Event-Based {{YOLO Object Detection}}},
  author = {Shariff, Waseem and Farooq, Muhammad Ali and Lemley, Joe and Corcoran, Peter},
  year = {2023},
  month = jan,
  number = {arXiv:2212.07181},
  eprint = {2212.07181},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.07181},
  urldate = {2025-05-30},
  abstract = {Neuromorphic vision or event vision is an advanced vision technology, where in contrast to the visible camera that outputs pixels, the event vision generates neuromorphic events every time there is a brightness change which exceeds a specific threshold in the field of view (FOV). This study focuses on leveraging neuromorphic event data for roadside object detection. This is a proof of concept towards building artificial intelligence (AI) based pipelines which can be used for forward perception systems for advanced vehicular applications. The focus is on building efficient state-of-the-art object detection networks with better inference results for fast-moving forward perception using an event camera. In this article, the event-simulated A2D2 dataset is manually annotated and trained on two different YOLOv5 networks (small and large variants). To further assess its robustness, single model testing and ensemble model testing are carried out.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/LTPJNYXI/Shariff et al. - 2023 - Event-based YOLO Object Detection Proof of Concept for Forward Perception System.pdf;/home/amit/Zotero/storage/BX7EHQK7/2212.html}
}

@article{vidalUltimateSLAMCombining2018,
  title = {Ultimate {{SLAM}}? {{Combining Events}}, {{Images}}, and {{IMU}} for {{Robust Visual SLAM}} in {{HDR}} and {{High Speed Scenarios}}},
  shorttitle = {Ultimate {{SLAM}}?},
  author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
  year = {2018},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {2},
  eprint = {1709.06310},
  primaryclass = {cs},
  pages = {994--1001},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2018.2793357},
  urldate = {2025-05-30},
  abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130\% over event-only pipelines, and 85\% over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Neuromorphic processing,SLAM},
  file = {/home/amit/Zotero/storage/DSFAFNHX/Vidal et al. - 2018 - Ultimate SLAM Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenar.pdf;/home/amit/Zotero/storage/UTESSM8F/1709.html}
}

@misc{iaboniEventbasedSpikingNeural2024,
  title = {Event-Based {{Spiking Neural Networks}} for {{Object Detection}}: {{A Review}} of {{Datasets}}, {{Architectures}}, {{Learning Rules}}, and {{Implementation}}},
  shorttitle = {Event-Based {{Spiking Neural Networks}} for {{Object Detection}}},
  author = {Iaboni, Craig and Abichandani, Pramod},
  year = {2024},
  month = nov,
  number = {arXiv:2411.17006},
  eprint = {2411.17006},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.17006},
  urldate = {2025-05-30},
  abstract = {Spiking Neural Networks (SNNs) represent a biologically inspired paradigm offering an energy-efficient alternative to conventional artificial neural networks (ANNs) for Computer Vision (CV) applications. This paper presents a systematic review of datasets, architectures, learning methods, implementation techniques, and evaluation methodologies used in CV-based object detection tasks using SNNs. Based on an analysis of 151 journal and conference articles, the review codifies: 1) the effectiveness of fully connected, convolutional, and recurrent architectures; 2) the performance of direct unsupervised, direct supervised, and indirect learning methods; and 3) the trade-offs in energy consumption, latency, and memory in neuromorphic hardware implementations. An open-source repository along with detailed examples of Python code and resources for building SNN models, event-based data processing, and SNN simulations are provided. Key challenges in SNN training, hardware integration, and future directions for CV applications are also identified.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/6ACK8TL7/Iaboni and Abichandani - 2024 - Event-based Spiking Neural Networks for Object Detection A Review of Datasets, Architectures, Learn.pdf;/home/amit/Zotero/storage/KSTTLRA6/2411.html}
}


@article{maassNetworksSpikingNeurons1997a,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  year = {1997},
  month = dec,
  journal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {08936080},
  doi = {10.1016/S0893-6080(97)00011-7},
  urldate = {2025-05-30},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. {\copyright} 1997 Elsevier Science Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/JBQH96PH/Maass - 1997 - Networks of spiking neurons The third generation of neural network models.pdf}
}


@online{bhattacharyaVisionTransformersEndtoEnd2025,
  title = {Vision {{Transformers}} for {{End-to-End Vision-Based Quadrotor Obstacle Avoidance}}},
  author = {Bhattacharya, Anish and Rao, Nishanth and Parikh, Dhruv and Kunapuli, Pratik and Wu, Yuwei and Tao, Yuezhan and Matni, Nikolai and Kumar, Vijay},
  date = {2025-04-01},
  eprint = {2405.10391},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.10391},
  url = {http://arxiv.org/abs/2405.10391},
  urldate = {2025-04-04},
  abstract = {We demonstrate the capabilities of an attention-based end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m/s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control.},
  pubstate = {prepublished},
  keywords = {.toread,Agile Flight,Computer Science - Artificial Intelligence,Computer Science - Robotics,Electrical Engineering and Systems Science - Image and Video Processing,important,toread},
  file = {/home/amit/Zotero/storage/X7YD9L7F/Bhattacharya et al. - 2025 - Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance.pdf;/home/amit/Zotero/storage/2ZKJRV7X/2405.html;/home/amit/Zotero/storage/9C9Z7X6E/vitfly.html}
}

@misc{romeroDreamFlyModelBased2025a,
  title = {Dream to {{Fly}}: {{Model-Based Reinforcement Learning}} for {{Vision-Based Drone Flight}}},
  shorttitle = {Dream to {{Fly}}},
  author = {Romero, Angel and Shenai, Ashwin and Geles, Ismail and Aljalbout, Elie and Scaramuzza, Davide},
  year = {2025},
  month = jan,
  number = {arXiv:2501.14377},
  eprint = {2501.14377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14377},
  urldate = {2025-05-30},
  abstract = {Autonomous drone racing has risen as a challenging robotic benchmark for testing the limits of learning, perception, planning, and control. Expert human pilots are able to agilely fly a drone through a race track by mapping the real-time feed from a single onboard camera directly to control commands. Recent works in autonomous drone racing attempting direct pixel-to-commands control policies (without explicit state estimation) have relied on either intermediate representations that simplify the observation space or performed extensive bootstrapping using Imitation Learning (IL). This paper introduces an approach that learns policies from scratch, allowing a quadrotor to autonomously navigate a race track by directly mapping raw onboard camera pixels to control commands, just as human pilots do. By leveraging model-based reinforcement learning{\textasciitilde}(RL) - specifically DreamerV3 - we train visuomotor policies capable of agile flight through a race track using only raw pixel observations. While model-free RL methods such as PPO struggle to learn under these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors. Moreover, because our policies learn directly from pixel inputs, the perception-aware reward term employed in previous RL approaches to guide the training process is no longer needed. Our experiments demonstrate in both simulation and real-world flight how the proposed approach can be deployed on agile quadrotors. This approach advances the frontier of vision-based autonomous flight and shows that model-based RL is a promising direction for real-world robotics.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/GHZRWZJR/Romero et al. - 2025 - Dream to Fly Model-Based Reinforcement Learning for Vision-Based Drone Flight.pdf;/home/amit/Zotero/storage/K52Y2VFV/2501.html}
}

@article{kaufmannChampionlevelDroneRacing2023,
  title = {Champion-Level Drone Racing Using Deep Reinforcement Learning},
  author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  date = {2023-08-31},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {620},
  number = {7976},
  pages = {982--987},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-023-06419-4},
  url = {https://www.nature.com/articles/s41586-023-06419-4},
  urldate = {2025-05-29},
  abstract = {Abstract                            First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors               1               . Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence               2               , which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  langid = {english},
  file = {/home/amit/Zotero/storage/WTHVDE7I/Kaufmann et al. - 2023 - Champion-level drone racing using deep reinforcement learning.pdf}
}

@misc{A_Comprehensive_Review_on_Leveraging_Machine_Learning_for_MultiAgent_Path_Findingpdf,
  title = {A\_{{Comprehensive}}\_{{Review}}\_on\_{{Leveraging}}\_{{Machine}}\_{{Learning}}\_for\_{{Multi-Agent}}\_{{Path}}\_{{Finding}}.Pdf},
  journal = {Google Docs},
  urldate = {2025-05-02},
  howpublished = {https://drive.google.com/file/d/1g9RAmkXh2wkf0cft9LTdlYdDnQVXxe1F/view?usp=sharing\&usp=embed\_facebook},
  keywords = {.toread,swarm},
  file = {/home/amit/Zotero/storage/8NV8SQJC/view.html}
}

@misc{ahmadPACNavCollectiveNavigation2023,
  title = {{{PACNav}}: {{A}} Collective Navigation Approach for {{UAV}} Swarms Deprived of Communication and External Localization},
  shorttitle = {{{PACNav}}},
  author = {Ahmad, Afzal and Licea, Daniel Bonilla and Silano, Giuseppe and Baca, Tomas and Saska, Martin},
  year = {2023},
  month = feb,
  eprint = {2302.05258},
  primaryclass = {cs},
  doi = {10.1088/1748-3190/ac98e6},
  urldate = {2025-04-30},
  abstract = {This article proposes Persistence Administered Collective Navigation (PACNav) as an approach for achieving decentralized collective navigation of Unmanned Aerial Vehicle (UAV) swarms. The technique is based on the flocking and collective navigation behavior observed in natural swarms, such as cattle herds, bird flocks, and even large groups of humans. As global and concurrent information of all swarm members is not available in natural swarms, these systems use local observations to achieve the desired behavior. Similarly, PACNav relies only on local observations of relative positions of UAVs, making it suitable for large swarms deprived of communication capabilities and external localization systems. We introduce the novel concepts of path persistence and path similarity that allow each swarm member to analyze the motion of other members in order to determine its own future motion. PACNav is based on two main principles: (1) UAVs with little variation in motion direction have high path persistence, and are considered by other UAVs to be reliable leaders; (2) groups of UAVs that move in a similar direction have high path similarity, and such groups are assumed to contain a reliable leader. The proposed approach also embeds a reactive collision avoidance mechanism to avoid collisions with swarm members and environmental obstacles. This collision avoidance ensures safety while reducing deviations from the assigned path. Along with several simulated experiments, we present a real-world experiment in a natural forest, showcasing the validity and effectiveness of the proposed collective navigation approach in challenging environments. The source code is released as open-source, making it possible to replicate the obtained results and facilitate the continuation of research by the community.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Multiagent Systems,Computer Science - Robotics,swarm},
  file = {/home/amit/Zotero/storage/NRLVCIZJ/Ahmad et al. - 2023 - PACNav A collective navigation approach for UAV swarms deprived of communication and external local.pdf;/home/amit/Zotero/storage/YR7S9YYI/2302.html}
}

@misc{atrcProfGuidoCroon2021,
  title = {Prof. {{Guido De Croon}} / {{The}} Artificial Intelligence behind Swarms of Autonomous Tiny Drones},
  author = {{ATRC}},
  year = {2021},
  month = mar,
  urldate = {2025-04-26},
  keywords = {swarm}
}

@misc{batraDecentralizedControlQuadrotor2021a,
  title = {Decentralized {{Control}} of {{Quadrotor Swarms}} with {{End-to-end Deep Reinforcement Learning}}},
  author = {Batra, Sumeet and Huang, Zhehui and Petrenko, Aleksei and Kumar, Tushar and Molchanov, Artem and Sukhatme, Gaurav S.},
  year = {2021},
  month = nov,
  number = {arXiv:2109.07735},
  eprint = {2109.07735},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.07735},
  urldate = {2025-04-24},
  abstract = {We demonstrate the possibility of learning drone swarm controllers that are zero-shot transferable to real quadrotors via large-scale multi-agent end-to-end reinforcement learning. We train policies parameterized by neural networks that are capable of controlling individual drones in a swarm in a fully decentralized manner. Our policies, trained in simulated environments with realistic quadrotor physics, demonstrate advanced flocking behaviors, perform aggressive maneuvers in tight formations while avoiding collisions with each other, break and re-establish formations to avoid collisions with moving obstacles, and efficiently coordinate in pursuit-evasion tasks. We analyze, in simulation, how different model architectures and parameters of the training regime influence the final performance of neural swarms. We demonstrate the successful deployment of the model learned in simulation to highly resource-constrained physical quadrotors performing station keeping and goal swapping behaviors. Code and video demonstrations are available on the project website at https://sites.google.com/view/swarm-rl.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,important,swarm},
  file = {/home/amit/Zotero/storage/GNY9HM68/Batra et al. - 2021 - Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning.pdf;/home/amit/Zotero/storage/D2IZF7M7/2109.html}
}

@misc{bhattacharyaMonocularEventBasedVision2024,
  title = {Monocular {{Event-Based Vision}} for {{Obstacle Avoidance}} with a {{Quadrotor}}},
  author = {Bhattacharya, Anish and Cannici, Marco and Rao, Nishanth and Tao, Yuezhan and Kumar, Vijay and Matni, Nikolai and Scaramuzza, Davide},
  year = {2024},
  month = nov,
  number = {arXiv:2411.03303},
  eprint = {2411.03303},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.03303},
  urldate = {2025-03-09},
  abstract = {We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.},
  archiveprefix = {arXiv},
  keywords = {Agile Flight,Computer Science - Robotics,event camera,important},
  file = {/home/amit/Zotero/storage/MSAW85B8/Bhattacharya et al. - 2024 - Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor.pdf;/home/amit/Zotero/storage/CDWIDJZA/2411.html}
}

@misc{bhattacharyaVisionTransformersEndtoEnd2025,
  title = {Vision {{Transformers}} for {{End-to-End Vision-Based Quadrotor Obstacle Avoidance}}},
  author = {Bhattacharya, Anish and Rao, Nishanth and Parikh, Dhruv and Kunapuli, Pratik and Wu, Yuwei and Tao, Yuezhan and Matni, Nikolai and Kumar, Vijay},
  year = {2025},
  month = apr,
  number = {arXiv:2405.10391},
  eprint = {2405.10391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.10391},
  urldate = {2025-04-04},
  abstract = {We demonstrate the capabilities of an attention-based end-to-end approach for high-speed vision-based quadrotor obstacle avoidance in dense, cluttered environments, with comparison to various state-of-the-art learning architectures. Quadrotor unmanned aerial vehicles (UAVs) have tremendous maneuverability when flown fast; however, as flight speed increases, traditional model-based approaches to navigation via independent perception, mapping, planning, and control modules breaks down due to increased sensor noise, compounding errors, and increased processing latency. Thus, learning-based, end-to-end vision-to-control networks have shown to have great potential for online control of these fast robots through cluttered environments. We train and compare convolutional, U-Net, and recurrent architectures against vision transformer (ViT) models for depth image-to-control in high-fidelity simulation, observing that ViT models are more effective than others as quadrotor speeds increase and in generalization to unseen environments, while the addition of recurrence further improves performance while reducing quadrotor energy cost across all tested flight speeds. We assess performance at speeds of up to 7m/s in simulation and hardware. To the best of our knowledge, this is the first work to utilize vision transformers for end-to-end vision-based quadrotor control.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Artificial Intelligence,Computer Science - Robotics,Electrical Engineering and Systems Science - Image and Video Processing,important,toread},
  file = {/home/amit/Zotero/storage/X7YD9L7F/Bhattacharya et al. - 2025 - Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance.pdf;/home/amit/Zotero/storage/2ZKJRV7X/2405.html;/home/amit/Zotero/storage/9C9Z7X6E/vitfly.html}
}

@misc{chakravarthiRecentEventCamera2024,
  title = {Recent {{Event Camera Innovations}}: {{A Survey}}},
  shorttitle = {Recent {{Event Camera Innovations}}},
  author = {Chakravarthi, Bharatesh and Verma, Aayush Atul and Daniilidis, Kostas and Fermuller, Cornelia},
  year = {2024},
  month = aug,
  number = {arXiv:2408.13627},
  eprint = {2408.13627},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2408.13627},
  urldate = {2025-03-20},
  abstract = {Event-based vision, inspired by the human visual system, offers transformative capabilities such as low latency, high dynamic range, and reduced power consumption. This paper presents a comprehensive survey of event cameras, tracing their evolution over time. It introduces the fundamental principles of event cameras, compares them with traditional frame cameras, and highlights their unique characteristics and operational differences. The survey covers various event camera models from leading manufacturers, key technological milestones, and influential research contributions. It explores diverse application areas across different domains and discusses essential real-world and synthetic datasets for research advancement. Additionally, the role of event camera simulators in testing and development is discussed. This survey aims to consolidate the current state of event cameras and inspire further innovation in this rapidly evolving field. To support the research community, a {\textbackslash}href\{https://github.com/chakravarthi589/Event-based-Vision\_Resources\}\{GitHub\} page categorizes past and future research articles and consolidates valuable resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Event Camera,important,Neuromorphic processing,Neuromorphic processor},
  file = {/home/amit/Zotero/storage/CJGF5WJ3/Chakravarthi et al. - 2024 - Recent Event Camera Innovations A Survey.pdf;/home/amit/Zotero/storage/MF8NXKM7/Chakravarthi et al. - 2024 - Recent Event Camera Innovations A Survey.pdf;/home/amit/Zotero/storage/X4H9U5TQ/2408.html}
}

@article{debieSwarmRoboticsSurvey2023,
  title = {Swarm {{Robotics}}: {{A Survey}} from a {{Multi-Tasking Perspective}}},
  shorttitle = {Swarm {{Robotics}}},
  author = {Debie, Essam and Kasmarik, Kathryn and Garratt, Matt},
  year = {2023},
  month = sep,
  journal = {ACM Comput. Surv.},
  volume = {56},
  number = {2},
  pages = {49:1--49:38},
  issn = {0360-0300},
  doi = {10.1145/3611652},
  urldate = {2025-05-17},
  abstract = {The behaviour of social insects such as bees and ants has influenced the development of swarm robots. To enable robots to cooperate together, swarm robotics employs principles such as communication, coordination, and collaboration. Collaboration among multiple robots can lead to a faster task completion time compared to the utilisation of a single, complex robot. One of the key aspects of swarm robotics is that control is distributed uniformly across the robots in the swarm, which boosts the system's resilience and fault tolerance. Through the use of the robots' embodied sensors and actuators, this distributed control often facilitates the emergence of collective behaviours through the interaction of the robots with one another and with the environment. The purpose of this survey is to examine the reasons behind the lack of utilisation of swarm robots in multi-tasking applications, which will be accomplished by studying previous research works in the field. We examine the literature from the perspective of multi-tasking: we pay particular attention to concepts that contribute to the progress of swarm robotics for multi-tasking applications. To do this, we first examine the different studies in multi-tasking swarm robotics, covering platforms, multi-tasking scenarios, sub-task allocation methodologies, and performance metrics. We then highlight several swarm robotics related disciplines that have significant effect on the development of swarm robotics for multi-tasking problems. We propose two taxonomies: the first categorises works based on the characteristics of the scenarios being handled, whereas the second taxonomy categorises works based on the swarming strategies utilised to achieve multi-tasking capabilities. We finish with a discussion of swarm robots' existing limitations for real-world multi-tasking applications, as well as recommendations for future research directions.},
  keywords = {.toread,important,Neuromorphic processing,swarm},
  file = {/home/amit/Zotero/storage/K5VFEW2F/_.pdf}
}

@misc{DistributedThreeDimensional,
  title = {Distributed {{Three Dimensional Flocking}} of {{Autonomous Drones}}},
  urldate = {2025-04-30},
  howpublished = {https://mrs.fel.cvut.cz/data/papers/Albani2022ICRA.pdf},
  keywords = {.toread,swarm},
  file = {/home/amit/Zotero/storage/KPEU4WGQ/Albani2022ICRA.pdf}
}

@misc{DistributedThreeDimensionala,
  title = {Distributed {{Three Dimensional Flocking}} of {{Autonomous Drones}}},
  urldate = {2025-05-02},
  howpublished = {https://mrs.fel.cvut.cz/data/papers/Albani2022ICRA.pdf},
  keywords = {.toread,Neuromorphic processing,swarm},
  file = {/home/amit/Zotero/storage/RR7KUXPA/Albani2022ICRA.pdf}
}

@article{duttaVisionGuidedOutdoorFlight,
  title = {Vision-{{Guided Outdoor Flight}} and {{Obstacle Evasion}} via {{Reinforcement Learning}}},
  author = {Dutta, Shiladitya and Gupta, Aayush and Saran, Varun and Zakhor, Avideh},
  abstract = {Although quadcopters boast impressive traversal capabilities enabled by their omnidirectional maneuverability, the need for continuous pilot control in complex environments impedes their application in GNSS and telemetry-denied scenarios. To this end, we propose a novel sensorimotor policy that uses stereo-vision depth and visual-inertial odometry (VIO) to autonomously navigate through obstacles in an unknown environment to reach a goal point. The policy is comprised of a pre-trained autoencoder as the perception head followed by a planning and control LSTM network which outputs velocity commands that can be followed by an off-the-shelf commercial drone. We leverage reinforcement and privileged learning paradigms to train the policy in simulation through a two-stage process: 1) initial training with optimal trajectories generated by a global motion planner acting as a supervisory backbone, 2) further fine-tuning in a curriculum environment. To bridge the sim-to-real gap, we employ domain randomization and reward shaping to create a policy that is both robust to noise and domain shift. In actual outdoor experiments, our approach achieves successful zero-shot transfer to both a drone platform (DJI M300) and environments with obstacles that were never encountered during training.},
  langid = {english},
  keywords = {.toread},
  file = {/home/amit/Zotero/storage/9F3GEWVC/Dutta et al. - Vision-Guided Outdoor Flight and Obstacle Evasion via Reinforcement Learning.pdf}
}

@misc{embeddedsystemsweekesweekESWEEK2021Education2021,
  title = {{{ESWEEK}} 2021 {{Education}} - {{Spiking Neural Networks}}},
  author = {{Embedded Systems Week (ESWEEK)}},
  year = {2021},
  month = nov,
  urldate = {2025-04-03},
  keywords = {.toread,important,Neuromorphic processing,nosource,Spiking Neural Networks}
}

@misc{eshraghianTrainingSpikingNeural2023,
  title = {Training {{Spiking Neural Networks Using Lessons From Deep Learning}}},
  author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  year = {2023},
  month = aug,
  number = {arXiv:2109.12894},
  eprint = {2109.12894},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.12894},
  urldate = {2025-05-19},
  abstract = {The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper serves as a tutorial and perspective showing how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks. We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks (SNNs); the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here. The fields of deep learning and spiking neural networks evolve very rapidly. We endeavour to treat this document as a 'dynamic' manuscript that will continue to be updated as the common practices in training SNNs also change. A series of companion interactive tutorials complementary to this paper using our Python package, snnTorch, are also made available. See https://snntorch.readthedocs.io/en/latest/tutorials/index.html .},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Emerging Technologies,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,important,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/6I8BL8CM/Eshraghian et al. - 2023 - Training Spiking Neural Networks Using Lessons From Deep Learning.pdf}
}

@article{feredeEndtoendNeuralNetwork2024,
  title = {End-to-End Neural Network Based Optimal Quadcopter Control},
  author = {Ferede, Robin and {de Croon}, Guido and De Wagter, Christophe and Izzo, Dario},
  year = {2024},
  month = feb,
  journal = {Robotics and Autonomous Systems},
  volume = {172},
  pages = {104588},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2023.104588},
  urldate = {2025-03-17},
  abstract = {Developing optimal controllers for aggressive high-speed quadcopter flight poses significant challenges in robotics. Recent trends in the field involve utilizing neural network controllers trained through supervised or reinforcement learning. However, the sim-to-real transfer introduces a reality gap, requiring the use of robust inner loop controllers during real flights, which limits the network's control authority and flight performance. In this paper, we investigate for the first time, an end-to-end neural network controller, addressing the reality gap issue without being restricted by an inner-loop controller. The networks, referred to as G\&CNets, are trained to learn an energy-optimal policy mapping the quadcopter's state to rpm commands using an optimal trajectory dataset. In hover-to-hover flights, we identified the unmodeled moments as a significant contributor to the reality gap. To mitigate this, we propose an adaptive control strategy that works by learning from optimal trajectories of a system affected by constant external pitch, roll and yaw moments. In real test flights, this model mismatch is estimated onboard and fed to the network to obtain the optimal rpm command. We demonstrate the effectiveness of our method by performing energy-optimal hover-to-hover flights with and without moment feedback. Finally, we compare the adaptive controller to a state-of-the-art differential-flatness-based controller in a consecutive waypoint flight and demonstrate the advantages of our method in terms of energy optimality and robustness.},
  keywords = {.toread,End-to-end control,G&CNet,Optimal control,Reality gap,Sim-to-real transfer,Supervised learning},
  file = {/home/amit/Zotero/storage/H7QYQSLU/S0921889023002270.html}
}

@inproceedings{feredeEndtoendReinforcementLearning2024,
  title = {End-to-End {{Reinforcement Learning}} for {{Time-Optimal Quadcopter Flight}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ferede, Robin and Wagter, Christophe De and Izzo, Dario and de Croon, Guido C. H. E.},
  year = {2024},
  month = may,
  eprint = {2311.16948},
  primaryclass = {cs},
  pages = {6172--6177},
  doi = {10.1109/ICRA57147.2024.10611665},
  urldate = {2025-04-04},
  abstract = {Aggressive time-optimal control of quadcopters poses a significant challenge in the field of robotics. The state-of-the-art approach leverages reinforcement learning (RL) to train optimal neural policies. However, a critical hurdle is the sim-to-real gap, often addressed by employing a robust inner loop controller -an abstraction that, in theory, constrains the optimality of the trained controller, necessitating margins to counter potential disturbances. In contrast, our novel approach introduces high-speed quadcopter control using end-to-end RL (E2E) that gives direct motor commands. To bridge the reality gap, we incorporate a learned residual model and an adaptive method that can compensate for modeling errors in thrust and moments. We compare our E2E approach against a state-of-the-art network that commands thrust and body rates to an INDI inner loop controller, both in simulated and real-world flight. E2E showcases a significant 1.39-second advantage in simulation and a 0.17-second edge in real-world testing, highlighting end-to-end reinforcement learning's potential. The performance drop observed from simulation to reality shows potential for further improvement, including refining strategies to address the reality gap or exploring offline reinforcement learning with real flight data.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/TSN2WC5Z/Ferede et al. - 2024 - End-to-end Reinforcement Learning for Time-Optimal Quadcopter Flight.pdf;/home/amit/Zotero/storage/WSZ6ZA25/2311.html}
}

@article{foehnAgiliciousOpensourceOpenhardware2022,
  title = {Agilicious: {{Open-source}} and Open-Hardware Agile Quadrotor for Vision-Based Flight},
  shorttitle = {Agilicious},
  author = {Foehn, Philipp and Kaufmann, Elia and Romero, Angel and Penicka, Robert and Sun, Sihao and Bauersfeld, Leonard and Laengle, Thomas and Cioffi, Giovanni and Song, Yunlong and Loquercio, Antonio and Scaramuzza, Davide},
  year = {2022},
  month = jun,
  journal = {Science Robotics},
  volume = {7},
  number = {67},
  pages = {eabl6259},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.abl6259},
  urldate = {2024-04-24},
  abstract = {Autonomous, agile quadrotor flight raises fundamental challenges for robotics research in terms of perception, planning, learning, and control. A versatile and standardized platform is needed to accelerate research and let practitioners focus on the core problems. To this end, we present Agilicious, a codesigned hardware and software framework tailored to autonomous, agile quadrotor flight. It is completely open source and open hardware and supports both model-based and neural network--based controllers. Also, it provides high thrust-to-weight and torque-to-inertia ratios for agility, onboard vision sensors, graphics processing unit (GPU)--accelerated compute hardware for real-time perception and neural network inference, a real-time flight controller, and a versatile software stack. In contrast to existing frameworks, Agilicious offers a unique combination of flexible software stack and high-performance hardware. We compare Agilicious with prior works and demonstrate it on different agile tasks, using both model-based and neural network--based controllers. Our demonstrators include trajectory tracking at up to 5               g               and 70 kilometers per hour in a motion capture system, and vision-based acrobatic flight and obstacle avoidance in both structured and unstructured environments using solely onboard perception. Last, we demonstrate its use for hardware-in-the-loop simulation in virtual reality environments. Because of its versatility, we believe that Agilicious supports the next generation of scientific and industrial quadrotor research.                        ,              We provide a codesigned hardware and software framework tailored to autonomous, agile quadrotor flight.},
  langid = {english},
  keywords = {Agile Flight},
  file = {/home/amit/Zotero/storage/B87HBQGR/Foehn et al. - 2022 - Agilicious Open-source and open-hardware agile qu.pdf;/home/amit/Zotero/storage/ZKV5VKD8/Foehn et al. - 2022 - Agilicious Open-source and open-hardware agile quadrotor for vision-based flight.pdf}
}

@article{gallegoEventbasedVisionSurvey2022,
  title = {Event-Based {{Vision}}: {{A Survey}}},
  shorttitle = {Event-Based {{Vision}}},
  author = {Gallego, Guillermo and Delbruck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew and Conradt, Joerg and Daniilidis, Kostas and Scaramuzza, Davide},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {1},
  eprint = {1904.08405},
  primaryclass = {cs},
  pages = {154--180},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.3008413},
  urldate = {2025-05-17},
  abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of {\textmu}s), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/KX6F3968/Gallego et al. - 2022 - Event-based Vision A Survey.pdf}
}

@misc{gelesDemonstratingAgileFlight2024,
  title = {Demonstrating {{Agile Flight}} from {{Pixels}} without {{State Estimation}}},
  author = {Geles, Ismail and Bauersfeld, Leonard and Romero, Angel and Xing, Jiaxu and Scaramuzza, Davide},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12505},
  eprint = {2406.12505},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12505},
  urldate = {2025-04-04},
  abstract = {Quadrotors are among the most agile flying robots. Despite recent advances in learning-based control and computer vision, autonomous drones still rely on explicit state estimation. On the other hand, human pilots only rely on a first-person-view video stream from the drone onboard camera to push the platform to its limits and fly robustly in unseen environments. To the best of our knowledge, we present the first vision-based quadrotor system that autonomously navigates through a sequence of gates at high speeds while directly mapping pixels to control commands. Like professional drone-racing pilots, our system does not use explicit state estimation and leverages the same control commands humans use (collective thrust and body rates). We demonstrate agile flight at speeds up to 40km/h with accelerations up to 2g. This is achieved by training vision-based policies with reinforcement learning (RL). The training is facilitated using an asymmetric actor-critic with access to privileged information. To overcome the computational complexity during image-based RL training, we use the inner edges of the gates as a sensor abstraction. This simple yet robust, task-relevant representation can be simulated during training without rendering images. During deployment, a Swin-transformer-based gate detector is used. Our approach enables autonomous agile flight with standard, off-the-shelf hardware. Although our demonstration focuses on drone racing, we believe that our method has an impact beyond drone racing and can serve as a foundation for future research into real-world applications in structured environments.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Robotics,important},
  file = {/home/amit/Zotero/storage/ARYH4JWM/Geles et al. - 2024 - Demonstrating Agile Flight from Pixels without State Estimation.pdf;/home/amit/Zotero/storage/VTMMHNSL/2406.html}
}

@article{horynaFastSwarmingUAVs2024,
  title = {Fast {{Swarming}} of {{UAVs}} in {{GNSS-Denied Feature-Poor Environments Without Explicit Communication}}},
  author = {Horyna, Ji{\v r}{\'i} and Kr{\'a}tk{\'y}, V{\'i}t and Pritzl, V{\'a}clav and B{\'a}{\v c}a, Tom{\'a}{\v s} and Ferrante, Eliseo and Saska, Martin},
  year = {2024},
  month = jun,
  journal = {IEEE Robotics and Automation Letters},
  volume = {9},
  number = {6},
  pages = {5284--5291},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2024.3390596},
  urldate = {2025-05-02},
  abstract = {A decentralized swarm approach for the fast cooperative flight of Unmanned Aerial Vehicles (UAVs) in feature-poor environments without any external localization and communication is introduced in this paper. A novel model of a UAV neighborhood is proposed to achieve robust onboard mutual perception and flocking state feedback control, which is designed to decrease the inter-agent oscillations common in standard reactive swarm models employed in fast collective motion. The novel swarming methodology is supplemented with an enhanced Multi-Robot State Estimation (MRSE) strategy to increase the reliability of the purely onboard localization, which may be unreliable in real environments. Although MRSE and the neighborhood model may rely on information exchange between agents, we introduce a communication-less version of the swarming framework based on estimating communicated states to decrease dependence on the often unreliable communication networks of large swarms. The proposed solution has been verified by a set of complex real-world experiments to demonstrate its overall capability in different conditions, including a UAV interception-motivated task with a group velocity reaching the physical limits of the individual hardware platforms.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {.toread,Neuromorphic processing,swarm},
  file = {/home/amit/Zotero/storage/NTZD5QJQ/Horyna et al. - 2024 - Fast Swarming of UAVs in GNSS-Denied Feature-Poor Environments Without Explicit Communication.pdf}
}

@misc{HttpsPuretudelftnlWs,
  title = {{{https://pure.tudelft.nl/ws/portalfiles/portal/143785276/electronics\_12\_00310.pdf}}},
  urldate = {2025-05-22},
  howpublished = {https://pure.tudelft.nl/ws/portalfiles/portal/143785276/electronics\_12\_00310.pdf},
  file = {/home/amit/Zotero/storage/59AT33QJ/electronics_12_00310.pdf}
}

@misc{HttpsPuretudelftnlWsa,
  title = {{{https://pure.tudelft.nl/ws/portalfiles/portal/143785276/electronics\_12\_00310.pdf}}},
  urldate = {2025-05-22},
  howpublished = {https://pure.tudelft.nl/ws/portalfiles/portal/143785276/electronics\_12\_00310.pdf},
  file = {/home/amit/Zotero/storage/QCK7PCUR/electronics_12_00310.pdf}
}

@misc{huangCollisionAvoidanceNavigation2024,
  title = {Collision {{Avoidance}} and {{Navigation}} for a {{Quadrotor Swarm Using End-to-end Deep Reinforcement Learning}}},
  author = {Huang, Zhehui and Yang, Zhaojing and Krupani, Rahul and {\c S}enba{\c s}lar, Bask{\i}n and Batra, Sumeet and Sukhatme, Gaurav S.},
  year = {2024},
  month = may,
  number = {arXiv:2309.13285},
  eprint = {2309.13285},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.13285},
  urldate = {2025-04-04},
  abstract = {End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80\% obstacle density in simulation and 8 robots with 20\% obstacle density in physical deployment. Video demonstrations are available on the project website at: https://sites.google.com/view/obst-avoid-swarm-rl.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,Computer Science - Robotics,important,swarm},
  file = {/home/amit/Zotero/storage/SZCJC5AK/Huang et al. - 2024 - Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learnin.pdf;/home/amit/Zotero/storage/KG2SJKL4/2309.html}
}

@misc{icaslabTrainingSpikingNeural2021,
  title = {Training {{Spiking Neural Networks Using Lessons From Deep Learning}}},
  author = {{iCAS Lab}},
  year = {2021},
  month = aug,
  urldate = {2025-04-27},
  keywords = {Neuromorphic processing}
}

@misc{incMilitaryDroneSwarm2024,
  title = {Military {{Drone Swarm Intelligence Explained}}},
  author = {Inc, Sentient Digital},
  year = {2024},
  month = apr,
  journal = {Sentient Digital, Inc.},
  urldate = {2025-04-07},
  abstract = {A military drone swarm can efficiently carry out missions while minimizing the danger to military personnel.},
  langid = {american},
  file = {/home/amit/Zotero/storage/T87A4SSC/military-drone-swarm-intelligence-explained.html}
}

@misc{jiangFullyAsynchronousNeuromorphic2024,
  title = {Fully {{Asynchronous Neuromorphic Perception}} for {{Mobile Robot Dodging}} with {{Loihi Chips}}},
  author = {Jiang, Junjie and Kong, Delei and Hu, Chenming and Fang, Zheng},
  year = {2024},
  month = dec,
  number = {arXiv:2410.10601},
  eprint = {2410.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10601},
  urldate = {2025-04-04},
  abstract = {Sparse and asynchronous sensing and processing in natural organisms lead to ultra low-latency and energy-efficient perception. Event cameras, known as neuromorphic vision sensors, are designed to mimic these characteristics. However, fully utilizing the sparse and asynchronous event stream remains challenging. Influenced by the mature algorithms of standard cameras, most existing event-based algorithms still rely on the ''group of events'' processing paradigm (e.g., event frames, 3D voxels) when handling event streams. This paradigm encounters issues such as feature loss, event stacking, and high computational burden, which deviates from the intended purpose of event cameras. To address these issues, we propose a fully asynchronous neuromorphic paradigm that integrates event cameras, spiking networks, and neuromorphic processors (Intel Loihi). This paradigm can faithfully process each event asynchronously as it arrives, mimicking the spike-driven signal processing in biological brains. We compare the proposed paradigm with the existing ''group of events'' processing paradigm in detail on the real mobile robot dodging task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {.toread,Computer Science - Robotics,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/T2Q62TSJ/Jiang et al. - 2024 - Fully Asynchronous Neuromorphic Perception for Mobile Robot Dodging with Loihi Chips.pdf}
}

@misc{jiangNeuroPlanner3DVisual2022,
  title = {Neuro-{{Planner}}: {{A 3D Visual Navigation Method}} for {{MAV}} with {{Depth Camera}} Based on {{Neuromorphic Reinforcement Learning}}},
  shorttitle = {Neuro-{{Planner}}},
  author = {Jiang, Junjie and Kong, Delei and Hou, Kuanxv and Huang, Xinjie and Zhuang, Hao and Zheng, Fang},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02305},
  eprint = {2210.02305},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02305},
  urldate = {2025-04-14},
  abstract = {Traditional visual navigation methods of micro aerial vehicle (MAV) usually calculate a passable path that satisfies the constraints depending on a prior map. However, these methods have issues such as high demand for computing resources and poor robustness in face of unfamiliar environments. Aiming to solve the above problems, we propose a neuromorphic reinforcement learning method (Neuro-Planner) that combines spiking neural network (SNN) and deep reinforcement learning (DRL) to realize MAV 3D visual navigation with depth camera. Specifically, we design spiking actor network based on two-state LIF (TS-LIF) neurons and its encoding-decoding schemes for efficient inference. Then our improved hybrid deep deterministic policy gradient (HDDPG) and TS-LIF-based spatio-temporal back propagation (STBP) algorithms are used as the training framework for actor-critic network architecture. To verify the effectiveness of the proposed Neuro-Planner, we carry out detailed comparison experiments with various SNN training algorithm (STBP, BPTT and SLAYER) in the software-in-the-loop (SITL) simulation framework. The navigation success rate of our HDDPG-STBP is 4.3{\textbackslash}\% and 5.3{\textbackslash}\% higher than that of the original DDPG in the two evaluation environments. To the best of our knowledge, this is the first work combining neuromorphic computing and deep reinforcement learning for MAV 3D visual navigation task.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Robotics,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/6XALIHLJ/Jiang et al. - 2022 - Neuro-Planner A 3D Visual Navigation Method for MAV with Depth Camera based on Neuromorphic Reinfor.pdf;/home/amit/Zotero/storage/5JS78CYL/2210.html}
}

@misc{levyDroneSwarmsCollective2025,
  title = {Drone {{Swarms}}: {{Collective Intelligence}} in {{Action}}},
  shorttitle = {Drone {{Swarms}}},
  author = {Levy, Jean-Jerome},
  year = {2025},
  month = feb,
  journal = {Scalastic},
  urldate = {2025-04-07},
  abstract = {Drone swarms are transforming warfare and entertainment. From Ukraine to China, discover their collective intelligence and applications.},
  chapter = {Technology},
  howpublished = {https://scalastic.io/en/drone-swarms-collective-intelligence/},
  langid = {english},
  file = {/home/amit/Zotero/storage/9SNVJXPE/drone-swarms-collective-intelligence.html}
}

@misc{liuSearchbasedMotionPlanning2017,
  title = {Search-Based {{Motion Planning}} for {{Aggressive Flight}} in {{SE}}(3)},
  author = {Liu, Sikang and Mohta, Kartik and Atanasov, Nikolay and Kumar, Vijay},
  year = {2017},
  month = oct,
  number = {arXiv:1710.02748},
  eprint = {1710.02748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.02748},
  urldate = {2025-04-08},
  abstract = {Quadrotors with large thrust-to-weight ratios are able to track aggressive trajectories with sharp turns and high accelerations. In this work, we develop a search-based trajectory planning approach that exploits the quadrotor maneuverability to generate sequences of motion primitives in cluttered environments. We model the quadrotor body as an ellipsoid and compute its flight attitude along trajectories in order to check for collisions against obstacles. The ellipsoid model allows the quadrotor to pass through gaps that are smaller than its diameter with non-zero pitch or roll angles. Without any prior information about the location of gaps and associated attitude constraints, our algorithm is able to find a safe and optimal trajectory that guides the robot to its goal as fast as possible. To accelerate planning, we first perform a lower dimensional search and use it as a heuristic to guide the generation of a final dynamically feasible trajectory. We analyze critical discretization parameters of motion primitive planning and demonstrate the feasibility of the generated trajectories in various simulations and real-world experiments.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Robotics,important,path-planning},
  file = {/home/amit/Zotero/storage/4TPXKS28/Liu et al. - 2017 - Search-based Motion Planning for Aggressive Flight in SE(3).pdf;/home/amit/Zotero/storage/5MAW5NX5/1710.html}
}

@article{liuSpikingNeuralNetworksBasedDataDriven,
  title = {Spiking {{Neural-Networks-Based Data-Driven Control}}},
  author = {Liu, Yuxiang and Pan, Wei},
  abstract = {Machine learning can be effectively applied in control loops to make optimal control decisions robustly. There is increasing interest in using spiking neural networks (SNNs) as the apparatus for machine learning in control engineering because SNNs can potentially offer high energy efficiency, and new SNN-enabling neuromorphic hardware is being rapidly developed. A defining characteristic of control problems is that environmental reactions and delayed rewards must be considered. Although reinforcement learning (RL) provides the fundamental mechanisms to address such problems, implementing these mechanisms in SNN learning has been underexplored. Previously, spike-timing-dependent plasticity learning schemes (STDP) modulated by factors of temporal difference (TD-STDP) or reward (R-STDP) have been proposed for RL with SNN. Here, we designed and implemented an SNN controller to explore and compare these two schemes by considering cart-pole balancing as a representative example. Although the TD-based learning rules are very general, the resulting model exhibits rather slow convergence, producing noisy and imperfect results even after prolonged training. We show that by integrating the understanding of the dynamics of the environment into the reward function of R-STDP, a robust SNN-based controller can be learned much more efficiently than TD-STDP.},
  langid = {english},
  file = {/home/amit/Zotero/storage/ZEIZKGDW/Liu and Pan - Spiking Neural-Networks-Based Data-Driven Control.pdf}
}

@article{loquercioLearningHighSpeedFlight2021,
  title = {Learning {{High-Speed Flight}} in the {{Wild}}},
  author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, Ren{\'e} and M{\"u}ller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  year = {2021},
  month = oct,
  journal = {Science Robotics},
  volume = {6},
  number = {59},
  eprint = {2110.05113},
  primaryclass = {cs},
  pages = {eabg5810},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.abg5810},
  urldate = {2025-04-04},
  abstract = {Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with on-board sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. While this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. Indeed, the subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and man-made environments at high speeds, with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.},
  archiveprefix = {arXiv},
  keywords = {Agile Flight,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,important},
  file = {/home/amit/Zotero/storage/5DVN2KWW/Learning high-speed flight in the wild.pdf;/home/amit/Zotero/storage/MJD2BWEL/Loquercio et al. - 2021 - Learning High-Speed Flight in the Wild.pdf;/home/amit/Zotero/storage/3ATNAZPF/scirobotics.html;/home/amit/Zotero/storage/ZW9XLPMD/2110.html}
}

@misc{mezeyPurelyVisionbasedCollective2024,
  title = {Purely Vision-Based Collective Movement of Robots},
  author = {Mezey, David and Bastien, Renaud and Zheng, Yating and McKee, Neal and Stoll, David and Hamann, Heiko and Romanczuk, Pawel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17106},
  eprint = {2406.17106},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.17106},
  urldate = {2025-04-07},
  abstract = {Collective movement inspired by animal groups promises inherited benefits for robot swarms, such as enhanced sensing and efficiency. However, while animals move in groups using only their local senses, robots often obey central control or use direct communication, introducing systemic weaknesses to the swarm. In the hope of addressing such vulnerabilities, developing bio-inspired decentralized swarms has been a major focus in recent decades. Yet, creating robots that move efficiently together using only local sensory information remains an extraordinary challenge. In this work, we present a decentralized, purely vision-based swarm of terrestrial robots. Within this novel framework robots achieve collisionless, polarized motion exclusively through minimal visual interactions, computing everything on board based on their individual camera streams, making central processing or direct communication obsolete. With agent-based simulations, we further show that using this model, even with a strictly limited field of view and within confined spaces, ordered group motion can emerge, while also highlighting key limitations. Our results offer a multitude of practical applications from hybrid societies coordinating collective movement without any common communication protocol, to advanced, decentralized vision-based robot swarms capable of diverse tasks in ever-changing environments.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Robotics,swarm},
  file = {/home/amit/Zotero/storage/YY7IBCMM/Mezey et al. - 2024 - Purely vision-based collective movement of robots.pdf;/home/amit/Zotero/storage/VMB3BRVZ/2406.html}
}

@article{muhsenSurveySwarmRobotics2024,
  title = {A {{Survey}} on {{Swarm Robotics}} for {{Area Coverage Problem}}},
  author = {Muhsen, Dena Kadhim and Sadiq, Ahmed T. and Raheem, Firas Abdulrazzaq},
  year = {2024},
  month = jan,
  journal = {Algorithms},
  volume = {17},
  number = {1},
  pages = {3},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  doi = {10.3390/a17010003},
  urldate = {2025-05-02},
  abstract = {The area coverage problem solution is one of the vital research areas which can benefit from swarm robotics. The greatest challenge to the swarm robotics system is to complete the task of covering an area effectively. Many domains where area coverage is essential include exploration, surveillance, mapping, foraging, and several other applications. This paper introduces a survey of swarm robotics in area coverage research papers from 2015 to 2022 regarding the algorithms and methods used, hardware, and applications in this domain. Different types of algorithms and hardware were dealt with and analysed; according to the analysis, the characteristics and advantages of each of them were identified, and we determined their suitability for different applications in covering the area for many goals. This study demonstrates that naturally inspired algorithms have the most significant role in swarm robotics for area coverage compared to other techniques. In addition, modern hardware has more capabilities suitable for supporting swarm robotics to cover an area, even if the environment is complex and contains static or dynamic obstacles.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {.toread,area coverage,hardware architecture,swarm,swarm robotics,swarm robotics algorithms},
  file = {/home/amit/Zotero/storage/L2E9HCPV/Muhsen et al. - 2024 - A Survey on Swarm Robotics for Area Coverage Problem.pdf}
}

@article{muirRoadCommercialSuccess2025,
  title = {The Road to Commercial Success for Neuromorphic Technologies},
  author = {Muir, Dylan Richard and Sheik, Sadique},
  year = {2025},
  month = apr,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {3586},
  issn = {2041-1723},
  doi = {10.1038/s41467-025-57352-1},
  urldate = {2025-05-02},
  abstract = {Neuromorphic technologies adapt biological neural principles to synthesise high-efficiency computational devices, characterised by continuous real-time operation and sparse event-based communication. After several false starts, a confluence of advances now promises widespread commercial adoption. Gradient-based training of deep spiking neural networks is now an off-the-shelf technique for building general-purpose Neuromorphic applications, with open-source tools underwritten by theoretical results. Analog and mixed-signal Neuromorphic circuit designs are being replaced by digital equivalents in newer devices, simplifying application deployment while maintaining computational benefits. Designs for in-memory computing are also approaching commercial maturity. Solving two key problems---how to program general Neuromorphic applications; and how to deploy them at scale---clears the way to commercial success of Neuromorphic processors. Ultra-low-power Neuromorphic technology will find a home in battery-powered systems, local compute for internet-of-things devices, and consumer wearables. Inspiration from uptake of tensor processors and GPUs can help the field overcome remaining hurdles.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {.toread,Computer science,Electrical and electronic engineering,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/D669WYKE/Muir and Sheik - 2025 - The road to commercial success for neuromorphic technologies.pdf}
}

@misc{Nengo,
  title = {Nengo},
  urldate = {2025-04-29},
  howpublished = {https://www.nengo.ai/},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/CLBBJM98/www.nengo.ai.html}
}

@misc{NeuromorphicComputingEngineering,
  title = {Neuromorphic {{Computing}} and {{Engineering}} with {{AI}} {\textbar} {{Intel}}{\textregistered}},
  journal = {Intel},
  urldate = {2025-04-29},
  abstract = {Discover how neuromorphic computing solutions represent the next wave of AI capabilities. See what neuromorphic chips and neural computers have to offer.},
  howpublished = {https://www.intel.com/content/www/us/en/research/neuromorphic-computing.html},
  langid = {english},
  keywords = {Neuromorphic processing},
  file = {/home/amit/Zotero/storage/L3EPNBQU/neuromorphic-computing.html}
}

@article{onizTrajectoryControlQuadrotors2024,
  title = {Trajectory {{Control}} of {{Quadrotors}} via {{Spiking Neural Networks}}},
  author = {Oniz, Yesim},
  year = {2024},
  month = jan,
  journal = {Electronics},
  volume = {13},
  number = {16},
  pages = {3319},
  issn = {2079-9292},
  doi = {10.3390/electronics13163319},
  urldate = {2025-05-11},
  abstract = {In this study, a novel control scheme based on spiking neural networks (SNNs) has been proposed to accomplish the trajectory tracking of quadrotor unmanned aerial vehicles (UAVs). The update rules for the network parameters have been derived using the Lyapunov stability theorem. Three different trajectories have been utilized in the simulated and experimental studies to verify the efficacy of the proposed control scheme. The acquired results have been compared with the responses obtained for proportional--integral--derivative (PID) and traditional neural network controllers. Simulated and experimental studies demonstrate that the proposed SNN-based controller is capable of providing better tracking accuracy and robust system response in the presence of disturbing factors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {.toread,important,Neuromorphic processing,rotary wing unmanned aerial vehicle,spiking neural networks,trajectory tracking},
  file = {/home/amit/Zotero/storage/JP7UJHN6/Oniz - 2024 - Trajectory Control of Quadrotors via Spiking Neural Networks.pdf}
}

@misc{paredes-vallesFullyNeuromorphicVision2023b,
  title = {Fully Neuromorphic Vision and Control for Autonomous Drone Flight},
  author = {{Paredes-Vall{\'e}s}, Federico and Hagenaars, Jesse and Dupeyroux, Julien and Stroobants, Stein and Xu, Yingfu and de Croon, Guido},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08778},
  eprint = {2303.08778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08778},
  urldate = {2025-04-04},
  abstract = {Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real event data. The control part consists of a single decoding layer and is learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone can accurately follow different ego-motion setpoints, allowing for hovering, landing, and maneuvering sideways\${\textbackslash}unicode\{x2014\}\$even while yawing at the same time. The neuromorphic pipeline runs on board on Intel's Loihi neuromorphic processor with an execution frequency of 200 Hz, spending only 27 \${\textbackslash}unicode\{x00b5\}\$J per inference. These results illustrate the potential of neuromorphic sensing and processing for enabling smaller, more intelligent robots.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,important,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/SCE2S9JN/Paredes-Vallés et al. - 2023 - Fully neuromorphic vision and control for autonomous drone flight.pdf;/home/amit/Zotero/storage/UNC42E66/2303.html}
}

@misc{pengObstacleAvoidanceResilient2022,
  title = {Obstacle {{Avoidance}} of {{Resilient UAV Swarm Formation}} with {{Active Sensing System}} in the {{Dense Environment}}},
  author = {Peng, Peng and Dong, Wei and Chen, Gang and Zhu, Xiangyang},
  year = {2022},
  month = feb,
  number = {arXiv:2202.13381},
  eprint = {2202.13381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.13381},
  urldate = {2025-04-07},
  abstract = {This paper proposes a perception-shared and swarm trajectory global optimal (STGO) algorithm fused UAVs formation motion planning framework aided by an active sensing system. First, the point cloud received by each UAV is fit by the gaussian mixture model (GMM) and transmitted in the swarm. Resampling from the received GMM contributes to a global map, which is used as the foundation for consensus. Second, to improve flight safety, an active sensing system is designed to plan the observation angle of each UAV considering the unknown field, overlap of the field of view (FOV), velocity direction and smoothness of yaw rotation, and this planning problem is solved by the distributed particle swarm optimization (DPSO) algorithm. Last, for the formation motion planning, to ensure obstacle avoidance, the formation structure is allowed for affine transformation and is treated as the soft constraint on the control points of the B-spline. Besides, the STGO is introduced to avoid local minima. The combination of GMM communication and STGO guarantees a safe and strict consensus between UAVs. Tests on different formations in the simulation show that our algorithm can contribute to a strict consensus and has a success rate of at least 80\% for obstacle avoidance in a dense environment. Besides, the active sensing system can increase the success rate of obstacle avoidance from 50\% to 100\% in some scenarios.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Robotics,swarm},
  file = {/home/amit/Zotero/storage/T8HNMFHM/Peng et al. - 2022 - Obstacle Avoidance of Resilient UAV Swarm Formation with Active Sensing System in the Dense Environm.pdf;/home/amit/Zotero/storage/YV4AS7QG/2202.html}
}

@article{penickaLearningMinimumTimeFlight2022,
  title = {Learning {{Minimum-Time Flight}} in {{Cluttered Environments}}},
  author = {Penicka, Robert and Song, Yunlong and Kaufmann, Elia and Scaramuzza, Davide},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {7209--7216},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3181755},
  urldate = {2025-04-04},
  abstract = {We tackle the problem of minimum-time flight for a quadrotor through a sequence of waypoints in the presence of obstacles while exploiting the full quadrotor dynamics. Early works relied on simplified dynamics or polynomial trajectory representations that did not exploit the full actuator potential of the quadrotor, and, thus, resulted in suboptimal solutions. Recent works can plan minimum-time trajectories; yet, the trajectories are executed with control methods that do not account for obstacles. Thus, a successful execution of such trajectories is prone to errors due to model mismatch and inflight disturbances. To this end, we leverage deep reinforcement learning and classical topological path planning to train robust neural-network controllers for minimum-time quadrotor flight in cluttered environments. The resulting neural network controller demonstrates substantially better performance of up to 19\% over state-of-the-art methods. More importantly, the learned policy solves the planning and control problem simultaneously online to account for disturbances, thus achieving much higher robustness. As such, the presented method achieves 100\% success rate of flying minimum-time policies without collision, while traditional planning and control approaches achieve only 40\%. The proposed method is validated in both simulation and the real world, with quadrotor speeds of up to 42 km h-1 and accelerations of 3.6g.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {.toread},
  file = {/home/amit/Zotero/storage/XMEK9EC9/Penicka et al. - 2022 - Learning Minimum-Time Flight in Cluttered Environments.pdf}
}

@misc{PsoAlgorithms_reviewpdf,
  title = {Pso Algorithms\_review.Pdf},
  journal = {Google Docs},
  urldate = {2025-05-02},
  howpublished = {https://drive.google.com/file/d/1k21jGzxRd4OmD5VIM7nQPTwRiVn9qFxI/view?usp=drive\_link\&usp=embed\_facebook},
  keywords = {.toread,swarm},
  file = {/home/amit/Zotero/storage/66FZA3PB/view.html}
}

@misc{romeroDreamFlyModelBased2025,
  title = {Dream to {{Fly}}: {{Model-Based Reinforcement Learning}} for {{Vision-Based Drone Flight}}},
  shorttitle = {Dream to {{Fly}}},
  author = {Romero, Angel and Shenai, Ashwin and Geles, Ismail and Aljalbout, Elie and Scaramuzza, Davide},
  year = {2025},
  month = jan,
  number = {arXiv:2501.14377},
  eprint = {2501.14377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14377},
  urldate = {2025-04-04},
  abstract = {Autonomous drone racing has risen as a challenging robotic benchmark for testing the limits of learning, perception, planning, and control. Expert human pilots are able to agilely fly a drone through a race track by mapping the real-time feed from a single onboard camera directly to control commands. Recent works in autonomous drone racing attempting direct pixel-to-commands control policies (without explicit state estimation) have relied on either intermediate representations that simplify the observation space or performed extensive bootstrapping using Imitation Learning (IL). This paper introduces an approach that learns policies from scratch, allowing a quadrotor to autonomously navigate a race track by directly mapping raw onboard camera pixels to control commands, just as human pilots do. By leveraging model-based reinforcement learning{\textasciitilde}(RL) - specifically DreamerV3 - we train visuomotor policies capable of agile flight through a race track using only raw pixel observations. While model-free RL methods such as PPO struggle to learn under these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors. Moreover, because our policies learn directly from pixel inputs, the perception-aware reward term employed in previous RL approaches to guide the training process is no longer needed. Our experiments demonstrate in both simulation and real-world flight how the proposed approach can be deployed on agile quadrotors. This approach advances the frontier of vision-based autonomous flight and shows that model-based RL is a promising direction for real-world robotics.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/8P2EASVC/Romero et al. - 2025 - Dream to Fly Model-Based Reinforcement Learning for Vision-Based Drone Flight.pdf;/home/amit/Zotero/storage/6338VPDD/2501.html}
}

@misc{rpgworkshopsGuidoCroonEventbased2021,
  title = {Guido de {{Croon}}. {{Event-based}} Vision and Processing for Tiny Drones},
  author = {{RPG Workshops}},
  year = {2021},
  month = jun,
  urldate = {2025-04-26},
  keywords = {Neuromorphic processing}
}

@misc{shaoulMultiRobotMotionPlanning2024,
  title = {Multi-{{Robot Motion Planning}} with {{Diffusion Models}}},
  author = {Shaoul, Yorai and Mishani, Itamar and Vats, Shivam and Li, Jiaoyang and Likhachev, Maxim},
  year = {2024},
  month = oct,
  number = {arXiv:2410.03072},
  eprint = {2410.03072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03072},
  urldate = {2025-04-05},
  abstract = {Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data. Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques -- generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. View video demonstrations in our supplementary material, and our code at: https://github.com/yoraish/mmd.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/QLTDEGB4/Shaoul et al. - 2024 - Multi-Robot Motion Planning with Diffusion Models.pdf;/home/amit/Zotero/storage/Q6AY993M/2410.html}
}

@misc{songLearningPerceptionAwareAgile2023,
  title = {Learning {{Perception-Aware Agile Flight}} in {{Cluttered Environments}}},
  author = {Song, Yunlong and Shi, Kexin and Penicka, Robert and Scaramuzza, Davide},
  year = {2023},
  month = mar,
  number = {arXiv:2210.01841},
  eprint = {2210.01841},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01841},
  urldate = {2025-04-04},
  abstract = {Recently, neural control policies have outperformed existing model-based planning-and-control methods for autonomously navigating quadrotors through cluttered environments in minimum time. However, they are not perception aware, a crucial requirement in vision-based navigation due to the camera's limited field of view and the underactuated nature of a quadrotor. We propose a learning-based system that achieves perception-aware, agile flight in cluttered environments. Our method combines imitation learning with reinforcement learning (RL) by leveraging a privileged learning-by-cheating framework. Using RL, we first train a perception-aware teacher policy with full-state information to fly in minimum time through cluttered environments. Then, we use imitation learning to distill its knowledge into a vision-based student policy that only perceives the environment via a camera. Our approach tightly couples perception and control, showing a significant advantage in computation speed (10 times faster) and success rate. We demonstrate the closed-loop control performance using hardware-in-the-loop simulation.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Artificial Intelligence,Computer Science - Robotics,important},
  file = {/home/amit/Zotero/storage/H8V5ZIZH/Song et al. - 2023 - Learning Perception-Aware Agile Flight in Cluttered Environments.pdf;/home/amit/Zotero/storage/QMT5BPM4/2210.html}
}

@misc{stroobantsNeuromorphicAttitudeEstimation2025,
  title = {Neuromorphic {{Attitude Estimation}} and {{Control}}},
  author = {Stroobants, Stein and de Wagter, Christophe and Croon, Guido C. H. E. De},
  year = {2025},
  month = mar,
  number = {arXiv:2411.13945},
  eprint = {2411.13945},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.13945},
  urldate = {2025-04-04},
  abstract = {The real-world application of small drones is mostly hampered by energy limitations. Neuromorphic computing promises extremely energy-efficient AI for autonomous flight but is still challenging to train and deploy on real robots. To reap the maximal benefits from neuromorphic computing, it is necessary to perform all autonomy functions end-to-end on a single neuromorphic chip, from low-level attitude control to high-level navigation. This research presents the first neuromorphic control system using a spiking neural network (SNN) to effectively map a drone's raw sensory input directly to motor commands. We apply this method to low-level attitude estimation and control for a quadrotor, deploying the SNN on a tiny Crazyflie. We propose a modular SNN, separately training and then merging estimation and control sub-networks. The SNN is trained with imitation learning, using a flight dataset of sensory-motor pairs. Post-training, the network is deployed on the Crazyflie, issuing control commands from sensor inputs at 500Hz. Furthermore, for the training procedure we augmented training data by flying a controller with additional excitation and time-shifting the target data to enhance the predictive capabilities of the SNN. On the real drone, the perception-to-control SNN tracks attitude commands with an average error of 3.0 degrees, compared to 2.7 degrees for the regular flight stack. We also show the benefits of the proposed learning modifications for reducing the average tracking error and reducing oscillations. Our work shows the feasibility of performing neuromorphic end-to-end control, laying the basis for highly energy-efficient and low-latency neuromorphic autopilots.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,important,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/EGCRV54D/Stroobants et al. - 2025 - Neuromorphic Attitude Estimation and Control.pdf;/home/amit/Zotero/storage/VM9TEYV4/2411.html}
}

@misc{Swarm_Robotics_Past_Present_and_Future_Point_of_Viewpdf,
  title = {Swarm\_{{Robotics}}\_{{Past}}\_{{Present}}\_and\_{{Future}}\_{{Point}}\_of\_{{View}}.Pdf},
  journal = {Google Docs},
  urldate = {2025-05-02},
  howpublished = {https://drive.google.com/file/d/18aop1RRczdyupBGMnC7JqmAEgj\_Bvp4c/view?usp=drive\_link\&usp=embed\_facebook},
  keywords = {.toread,swarm},
  file = {/home/amit/Zotero/storage/YTN2AG6W/view.html}
}

@misc{tajbakhshConflictBasedModelPredictive2024,
  title = {Conflict-{{Based Model Predictive Control}} for {{Scalable Multi-Robot Motion Planning}}},
  author = {Tajbakhsh, Ardalan and Biegler, Lorenz T. and Johnson, Aaron M.},
  year = {2024},
  month = apr,
  number = {arXiv:2303.01619},
  eprint = {2303.01619},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.01619},
  urldate = {2025-05-09},
  abstract = {This paper presents a scalable multi-robot motion planning algorithm called Conflict-Based Model Predictive Control (CB-MPC). Inspired by Conflict-Based Search (CBS), the planner leverages a similar high-level conflict tree to efficiently resolve robot-robot conflicts in the continuous space, while reasoning about each agent's kinematic and dynamic constraints and actuation limits using MPC as the low-level planner. We show that tracking high-level multi-robot plans with a vanilla MPC controller is insufficient, and results in unexpected collisions in tight navigation scenarios. Compared to other variations of multi-robot MPC like joint, prioritized, and distributed, we demonstrate that CB-MPC improves the executability and success rate, allows for closer robot-robot interactions, and reduces the computational cost significantly without compromising the solution quality across a variety of environments. Furthermore, we show that CB-MPC combined with a high-level path planner can effectively substitute computationally expensive full-horizon multi-robot kinodynamic planners.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Multiagent Systems,Computer Science - Robotics,swarm},
  file = {/home/amit/Zotero/storage/9P2Y2QMN/Tajbakhsh et al. - 2024 - Conflict-Based Model Predictive Control for Scalable Multi-Robot Motion Planning.pdf}
}

@misc{uzhroboticsandperceptiongroupAgileAerialAutonomy,
  title = {Agile {{Aerial Autonomy}}: {{Planning}} and {{Control}} ({{PhD Defense}} of {{Philipp Foehn}})},
  shorttitle = {Agile {{Aerial Autonomy}}},
  author = {{UZH Robotics {and} Perception Group}},
  urldate = {2025-04-27}
}

@article{vandijkVisualRouteFollowing2024,
  title = {Visual Route Following for Tiny Autonomous Robots},
  author = {Van Dijk, Tom and De Wagter, Christophe and De Croon, Guido C. H. E.},
  year = {2024},
  month = jul,
  journal = {Science Robotics},
  volume = {9},
  number = {92},
  pages = {eadk0310},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.adk0310},
  urldate = {2025-04-04},
  abstract = {Navigation is an essential capability for autonomous robots. In particular, visual navigation has been a major research topic in robotics because cameras are lightweight, power-efficient sensors that provide rich information on the environment. However, the main challenge of visual navigation is that it requires substantial computational power and memory for visual processing and storage of the results. As of yet, this has precluded its use on small, extremely resource-constrained robots such as lightweight drones. Inspired by the parsimony of natural intelligence, we propose an insect-inspired approach toward visual navigation that is specifically aimed at extremely resource-restricted robots. It is a route-following approach in which a robot's outbound trajectory is stored as a collection of highly compressed panoramic images together with their spatial relationships as measured with odometry. During the inbound journey, the robot uses a combination of odometry and visual homing to return to the stored locations, with visual homing preventing the buildup of odometric drift. A main advancement of the proposed strategy is that the number of stored compressed images is minimized by spacing them apart as far as the accuracy of odometry allows. To demonstrate the suitability for small systems, we implemented the strategy on a tiny 56-gram drone. The drone could successfully follow routes up to 100 meters with a trajectory representation that consumed less than 20 bytes per meter. The presented method forms a substantial step toward the autonomous visual navigation of tiny robots, facilitating their more widespread application.           ,              A 56g drone uses odometry and visual homing for visual route following.           ,              Editor's summary                            For navigation, robots often rely either on external infrastructure or on mapping-based navigation algorithms that are generally associated with high computational demands. van Dijk               et al               . have now developed an insect-inspired approach for visual navigation of resource-constrained miniature drones by exploiting both visual homing and odometry. The framework was tested on a 56-g drone, which was shown to be capable of following routes in indoor environments over long distances with minimal memory requirements. This approach could substantially improve the autonomous navigation of aerial robots with constraints in computational power, energy demands, and weight. ---Amos Matsiko},
  langid = {english},
  keywords = {.toread},
  file = {/home/amit/Zotero/storage/G9KR9QMS/Van Dijk et al. - 2024 - Visual route following for tiny autonomous robots.pdf}
}

@misc{vitaleEventdrivenVisionControl2021,
  title = {Event-Driven {{Vision}} and {{Control}} for {{UAVs}} on a {{Neuromorphic Chip}}},
  author = {Vitale, Antonio and Renner, Alpha and Nauer, Celine and Scaramuzza, Davide and Sandamirskaya, Yulia},
  year = {2021},
  month = aug,
  number = {arXiv:2108.03694},
  eprint = {2108.03694},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.03694},
  urldate = {2025-04-14},
  abstract = {Event-based vision sensors achieve up to three orders of magnitude better speed vs. power consumption trade off in high-speed control of UAVs compared to conventional image sensors. Event-based cameras produce a sparse stream of events that can be processed more efficiently and with a lower latency than images, enabling ultra-fast vision-driven control. Here, we explore how an event-based vision algorithm can be implemented as a spiking neuronal network on a neuromorphic chip and used in a drone controller. We show how seamless integration of event-based perception on chip leads to even faster control rates and lower latency. In addition, we demonstrate how online adaptation of the SNN controller can be realised using on-chip learning. Our spiking neuronal network on chip is the first example of a neuromorphic vision-based controller solving a high-speed UAV control task. The excellent scalability of processing in neuromorphic hardware opens the possibility to solve more challenging visual tasks in the future and integrate visual perception in fast control loops.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Artificial Intelligence,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,important,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/IRTFE833/Vitale et al. - 2021 - Event-driven Vision and Control for UAVs on a Neuromorphic Chip.pdf;/home/amit/Zotero/storage/FHKGKWKA/2108.html}
}

@misc{wangDashingGoldenSnitch2025,
  title = {Dashing for the {{Golden Snitch}}: {{Multi-Drone Time-Optimal Motion Planning}} with {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {Dashing for the {{Golden Snitch}}},
  author = {Wang, Xian and Zhou, Jin and Feng, Yuanli and Mei, Jiahao and Chen, Jiming and Li, Shuo},
  year = {2025},
  month = mar,
  number = {arXiv:2409.16720},
  eprint = {2409.16720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.16720},
  urldate = {2025-04-04},
  abstract = {Recent innovations in autonomous drones have facilitated time-optimal flight in single-drone configurations, and enhanced maneuverability in multi-drone systems by applying optimal control and learning-based methods. However, few studies have achieved time-optimal motion planning for multi-drone systems, particularly during highly agile maneuvers or in dynamic scenarios. This paper presents a decentralized policy network using multi-agent reinforcement learning for time-optimal multi-drone flight. To strike a balance between flight efficiency and collision avoidance, we introduce a soft collision-free mechanism inspired by optimization-based methods. By customizing PPO in a centralized training, decentralized execution (CTDE) fashion, we unlock higher efficiency and stability in training while ensuring lightweight implementation. Extensive simulations show that, despite slight performance trade-offs compared to single-drone systems, our multi-drone approach maintains near-time-optimal performance with a low collision rate. Real-world experiments validate our method, with two quadrotors using the same network as in simulation achieving a maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m * 5.5 m * 2.0 m space across various tracks, relying entirely on onboard computation.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Machine Learning,Computer Science - Robotics,important,swarm},
  file = {/home/amit/Zotero/storage/CLY6AVAM/Wang et al. - 2025 - Dashing for the Golden Snitch Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforceme.pdf;/home/amit/Zotero/storage/L4YIM9Y7/2409.html}
}

@article{wangMotionPlanningMethod2023,
  title = {Motion {{Planning Method}} for {{Car-Like Autonomous Mobile Robots}} in {{Dynamic Obstacle Environments}}},
  author = {Wang, Zhiwei and Li, Peiqing and Li, Qipeng and Wang, Zhongshan and Li, Zhuoran},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {137387--137400},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3339539},
  urldate = {2025-03-03},
  abstract = {Motion planning between dynamic obstacles is an essential capability to achieve real-world navigation. In this study, we investigated the problem of avoiding dynamic obstacles in complex environments for a car-like mobile robot with an incompletely constrained Ackerman front wheel steering. To address the problems of weak dynamic obstacle avoidance and poor path smoothing in motion planning with the traditional Timed Elastic Band (TEB) algorithm, We proposed a hybrid motion planning algorithm (TEB-CA,Timed Elastic Band-Collision Avoidance) that combines an improved traditional TEB algorithm and Optimal Reciprocal Collision Avoidance (ORCA) model to improve the ability of the robot to predict dynamic obstacles in advance and avoid collisions safely. Moreover, We also add new constraints to the traditional TEB algorithm, including: jerk constraints, smoothness constraints, and curvature constraints. The algorithm is implemented in C++ and evaluated experimentally in the Gazebo and Rviz simulation environments of the Robot Operating System (ROS), as well as in actual experimental tests on our car-like autonomous mobile robot ``Little Ant'' which proves the effectiveness of the method, and that the motion planning scheme is more effective in avoiding dynamic obstacles than the traditional TEB and DWA algorithms.},
  keywords = {Car-like autonomous mobile robot,Collision avoidance,dynamic obstacle avoidance,Dynamics,Heuristic algorithms,Mobile robots,Motion planning,nosource,optimal reciprocal collision avoidance,Planning,Robots,timed elastic band,Vehicle dynamics}
}

@article{yangEvGNNEventdrivenGraph2025,
  title = {{{EvGNN}}: {{An Event-driven Graph Neural Network Accelerator}} for {{Edge Vision}}},
  shorttitle = {{{EvGNN}}},
  author = {Yang, Yufeng and Kneip, Adrian and Frenkel, Charlotte},
  year = {2025},
  month = mar,
  journal = {IEEE Transactions on Circuits and Systems for Artificial Intelligence},
  volume = {2},
  number = {1},
  eprint = {2404.19489},
  primaryclass = {cs},
  pages = {37--50},
  issn = {2996-6647},
  doi = {10.1109/TCASAI.2024.3520905},
  urldate = {2025-04-05},
  abstract = {Edge vision systems combining sensing and embedded processing promise low-latency, decentralized, and energy-efficient solutions that forgo reliance on the cloud. As opposed to conventional frame-based vision sensors, event-based cameras deliver a microsecond-scale temporal resolution with sparse information encoding, thereby outlining new opportunities for edge vision systems. However, mainstream algorithms for frame-based vision, which mostly rely on convolutional neural networks (CNNs), can hardly exploit the advantages of event-based vision as they are typically optimized for dense matrix-vector multiplications. While event-driven graph neural networks (GNNs) have recently emerged as a promising solution for sparse event-based vision, their irregular structure is a challenge that currently hinders the design of efficient hardware accelerators. In this paper, we propose EvGNN, the first event-driven GNN accelerator for low-footprint, ultra-low-latency, and high-accuracy edge vision with event-based cameras. It relies on three central ideas: (i) directed dynamic graphs exploiting single-hop nodes with edge-free storage, (ii) event queues for the efficient identification of local neighbors within a spatiotemporally decoupled search range, and (iii) a novel layer-parallel processing scheme allowing for a low-latency execution of multi-layer GNNs. We deployed EvGNN on a Xilinx KV260 Ultrascale+ MPSoC platform and benchmarked it on the N-CARS dataset for car recognition, demonstrating a classification accuracy of 87.8\% and an average latency per event of 16\${\textbackslash}mu\$s, thereby enabling real-time, microsecond-resolution event-based vision at the edge.},
  archiveprefix = {arXiv},
  keywords = {.toread,broken,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Emerging Technologies,Computer Science - Hardware Architecture,Computer Science - Neural and Evolutionary Computing,Neuromorphic processing},
  file = {/home/amit/Zotero/storage/FEW5U7ZJ/Yang et al. - 2025 - EvGNN An Event-driven Graph Neural Network Accelerator for Edge Vision.pdf;/home/amit/Zotero/storage/TMFZLPQH/Yang et al. - 2025 - EvGNN An Event-driven Graph Neural Network Accelerator for Edge Vision.pdf;/home/amit/Zotero/storage/TKBTIJS8/2404.html}
}

@misc{yuMAVRLLearnFly2024,
  title = {{{MAVRL}}: {{Learn}} to {{Fly}} in {{Cluttered Environments}} with {{Varying Speed}}},
  shorttitle = {{{MAVRL}}},
  author = {Yu, Hang and Wagter, Christophe De and de Croon, Guido C. H. E.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08381},
  eprint = {2402.08381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08381},
  urldate = {2025-03-20},
  abstract = {Many existing obstacle avoidance algorithms overlook the crucial balance between safety and agility, especially in environments of varying complexity. In our study, we introduce an obstacle avoidance pipeline based on reinforcement learning. This pipeline enables drones to adapt their flying speed according to the environmental complexity. Moreover, to improve the obstacle avoidance performance in cluttered environments, we propose a novel latent space. The latent space in this representation is explicitly trained to retain memory of previous depth map observations. Our findings confirm that varying speed leads to a superior balance of success rate and agility in cluttered environments. Additionally, our memory-augmented latent representation outperforms the latent representation commonly used in reinforcement learning. Finally, after minimal fine-tuning, we successfully deployed our network on a real drone for enhanced obstacle avoidance.},
  archiveprefix = {arXiv},
  keywords = {.toread,Agile Flight,Computer Science - Robotics},
  file = {/home/amit/Zotero/storage/6HAHTVN8/Yu et al. - 2024 - MAVRL Learn to Fly in Cluttered Environments with Varying Speed.pdf;/home/amit/Zotero/storage/TXZKMAHA/2402.html}
}

@article{zanattaExploringSpikingNeural2024,
  title = {Exploring Spiking Neural Networks for Deep Reinforcement Learning in Robotic Tasks},
  author = {Zanatta, Luca and Barchi, Francesco and Manoni, Simone and Tolu, Silvia and Bartolini, Andrea and Acquaviva, Andrea},
  year = {2024},
  month = dec,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {30648},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-77779-8},
  urldate = {2025-04-29},
  abstract = {Spiking Neural Networks (SNNs) stand as the third generation of Artificial Neural Networks (ANNs), mirroring the functionality of the mammalian brain more closely than their predecessors. Their computational units, spiking neurons, characterized by Ordinary Differential Equations (ODEs), allow for dynamic system representation, with spikes serving as the medium for asynchronous communication among neurons. Due to their inherent ability to capture input dynamics, SNNs hold great promise for deep networks in Reinforcement Learning (RL) tasks. Deep RL (DRL), and in particular Proximal Policy Optimization (PPO) has been proven to be valuable for training robots due to the difficulty in creating comprehensive offline datasets that capture all environmental features. DRL combined with SNNs offers a compelling solution for tasks characterized by temporal complexity. In this work, we study the effectiveness of SNNs on DRL tasks leveraging a novel framework we developed for training SNNs with PPO in the Isaac Gym simulator implemented using the skrl library. Thanks to its significantly faster training speed compared to available SNN DRL tools, the framework allowed us to: (i) Perform an effective exploration of SNN configurations for DRL robotic tasks; (ii) Compare SNNs and ANNs for various network configurations such as the number of layers and neurons. Our work demonstrates that in DRL tasks the optimal SNN topology has a lower number of layers than ANN and we highlight how the state-of-art SNN architectures used in complex RL tasks, such as Ant, SNNs have difficulties fully leveraging deeper layers. Finally, we applied the best topology identified thanks to our Isaac Gym-based framework on Ant-v4 benchmark running on MuJoCo simulator, exhibiting a performance improvement by a factor of 4.4\$\${\textbackslash}times\$\$over the state-of-art SNN trained on the same task.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {.toread,Computer science,important,Neuromorphic processing,Neuromporphic processing,RL,Software},
  file = {/home/amit/Zotero/storage/C3QWAHEE/Zanatta et al. - 2024 - Exploring spiking neural networks for deep reinforcement learning in robotic tasks.pdf}
}

@article{zanattaExploringSpikingNeural2024a,
  title = {Exploring Spiking Neural Networks for Deep Reinforcement Learning in Robotic Tasks},
  author = {Zanatta, Luca and Barchi, Francesco and Manoni, Simone and Tolu, Silvia and Bartolini, Andrea and Acquaviva, Andrea},
  year = {2024},
  month = dec,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {30648},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-77779-8},
  urldate = {2025-05-15},
  abstract = {Spiking Neural Networks (SNNs) stand as the third generation of Artificial Neural Networks (ANNs), mirroring the functionality of the mammalian brain more closely than their predecessors. Their computational units, spiking neurons, characterized by Ordinary Differential Equations (ODEs), allow for dynamic system representation, with spikes serving as the medium for asynchronous communication among neurons. Due to their inherent ability to capture input dynamics, SNNs hold great promise for deep networks in Reinforcement Learning (RL) tasks. Deep RL (DRL), and in particular Proximal Policy Optimization (PPO) has been proven to be valuable for training robots due to the difficulty in creating comprehensive offline datasets that capture all environmental features. DRL combined with SNNs offers a compelling solution for tasks characterized by temporal complexity. In this work, we study the effectiveness of SNNs on DRL tasks leveraging a novel framework we developed for training SNNs with PPO in the Isaac Gym simulator implemented using the skrl library. Thanks to its significantly faster training speed compared to available SNN DRL tools, the framework allowed us to: (i) Perform an effective exploration of SNN configurations for DRL robotic tasks; (ii) Compare SNNs and ANNs for various network configurations such as the number of layers and neurons. Our work demonstrates that in DRL tasks the optimal SNN topology has a lower number of layers than ANN and we highlight how the state-of-art SNN architectures used in complex RL tasks, such as Ant, SNNs have difficulties fully leveraging deeper layers. Finally, we applied the best topology identified thanks to our Isaac Gym-based framework on Ant-v4 benchmark running on MuJoCo simulator, exhibiting a performance improvement by a factor of 4.4\$\${\textbackslash}times\$\$over the state-of-art SNN trained on the same task.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {.toread,Computer science,important,Neuromorphic processing,Software},
  file = {/home/amit/Zotero/storage/GCYKCVBG/Zanatta et al. - 2024 - Exploring spiking neural networks for deep reinforcement learning in robotic tasks.pdf}
}

@article{zhaoNatureinspiredSelforganizingCollision2022,
  title = {Nature-Inspired Self-Organizing Collision Avoidance for Drone Swarm Based on Reward-Modulated Spiking Neural Network},
  author = {Zhao, Feifei and Zeng, Yi and Han, Bing and Fang, Hongjian and Zhao, Zhuoya},
  year = {2022},
  month = nov,
  journal = {Patterns},
  volume = {3},
  number = {11},
  pages = {100611},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2022.100611},
  urldate = {2025-04-29},
  abstract = {Biological systems can exhibit intelligent swarm behavior through relatively independent individual, local interaction and decentralized decision-making. A major research challenge of self-organized swarm intelligence is the coupling influences between individual behaviors. Existing methods optimize the behavior of multiple individuals simultaneously from a global perspective. However, these methods lack in-depth inspiration from swarm behaviors in nature, so they are short of flexibly adapting to real multi-robot online decision-making tasks. To overcome such limits, this paper proposes a self-organized collision avoidance model for real drones incorporating a bio-inspired reward-modulated spiking neural network (RSNN). The local interaction and autonomous learning of a single individual leads to the emergence of swarm intelligence. We validated the proposed model on swarm collision avoidance tasks (a swarm of unmanned aerial vehicles without central control) in a bounded space, carrying out simulation and real-world experiments. Compared with artificial neural network-based online learning methods, our proposed method exhibits superior performance and better stability.},
  keywords = {.toread,and tested for one domain/problem,decentralized decision-making,DSML 2: Proof-of-concept: Data science output has been formulated,implemented,important,local interactions,Neuromorphic processing,Neuromporphic processing,reward-modulated spiking neural network,self-organizing collaboration,swarm,swarm intelligence emergence},
  file = {/home/amit/Zotero/storage/TLX29RD6/Zhao et al. - 2022 - Nature-inspired self-organizing collision avoidance for drone swarm based on reward-modulated spikin.pdf;/home/amit/Zotero/storage/IL34TUDM/S2666389922002367.html}
}

@article{zhouSwarmMicroFlying2022,
  title = {Swarm of Micro Flying Robots in the Wild},
  author = {Zhou, Xin and Wen, Xiangyong and Wang, Zhepei and Gao, Yuman and Li, Haojia and Wang, Qianhao and Yang, Tiankai and Lu, Haojian and Cao, Yanjun and Xu, Chao and Gao, Fei},
  year = {2022},
  month = may,
  journal = {Science Robotics},
  volume = {7},
  number = {66},
  pages = {eabm5954},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.abm5954},
  urldate = {2025-04-05},
  abstract = {Aerial robots are widely deployed, but highly cluttered environments such as dense forests remain inaccessible to drones and even more so to swarms of drones. In these scenarios, previously unknown surroundings and narrow corridors combined with requirements of swarm coordination can create challenges. To enable swarm navigation in the wild, we develop miniature but fully autonomous drones with a trajectory planner that can function in a timely and accurate manner based on limited information from onboard sensors. The planning problem satisfies various task requirements including flight efficiency, obstacle avoidance, and inter-robot collision avoidance, dynamical feasibility, swarm coordination, and so on, thus realizing an extensible planner. Furthermore, the proposed planner deforms trajectory shapes and adjusts time allocation synchronously based on spatial-temporal joint optimization. A high-quality trajectory thus can be obtained after exhaustively exploiting the solution space within only a few milliseconds, even in the most constrained environment. The planner is finally integrated into the developed palm-sized swarm platform with onboard perception, localization, and control. Benchmark comparisons validate the superior performance of the planner in trajectory quality and computing time. Various real-world field experiments demonstrate the extensibility of our system. Our approach evolves aerial robotics in three aspects: capability of cluttered environment navigation, extensibility to diverse task requirements, and coordination as a swarm without external facilities.           ,              A fully autonomous swarm composed of palm-sized drones with versatile task extensibility in the wild is realized.},
  langid = {english},
  keywords = {.toread,important,swarm},
  file = {/home/amit/Zotero/storage/SKHUX7NI/Zhou et al. - 2022 - Swarm of micro flying robots in the wild.pdf}
}

@misc{zhuSpikingNeuralNetworks2024,
  title = {Spiking {{Neural Networks}} as a {{Controller}} for {{Emergent Swarm Agents}}},
  author = {Zhu, Kevin and Mattson, Connor and Snyder, Shay and Vega, Ricardo and Brown, Daniel S. and Parsa, Maryam and Nowzari, Cameron},
  year = {2024},
  month = oct,
  number = {arXiv:2410.16175},
  eprint = {2410.16175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.16175},
  urldate = {2025-04-29},
  abstract = {Drones which can swarm and loiter in a certain area cost hundreds of dollars, but mosquitos can do the same and are essentially worthless. To control swarms of low-cost robots, researchers may end up spending countless hours brainstorming robot configurations and policies to ``organically" create behaviors which do not need expensive sensors and perception. Existing research explores the possible emergent behaviors in swarms of robots with only a binary sensor and a simple but hand-picked controller structure. Even agents in this highly limited sensing, actuation, and computational capability class can exhibit relatively complex global behaviors such as aggregation, milling, and dispersal, but finding the local interaction rules that enable more collective behaviors remains a significant challenge. This paper investigates the feasibility of training spiking neural networks to find those local interaction rules that result in particular emergent behaviors. In this paper, we focus on simulating a specific milling behavior already known to be producible using very simple binary sensing and acting agents. To do this, we use evolutionary algorithms to evolve not only the parameters (the weights, biases, and delays) of a spiking neural network, but also its structure. To create a baseline, we also show an evolutionary search strategy over the parameters for the incumbent hand-picked binary controller structure. Our simulations show that spiking neural networks can be evolved in binary sensing agents to form a mill.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,important,Neuromorphic processing,swarm},
  file = {/home/amit/Zotero/storage/34G5HU2M/Zhu et al. - 2024 - Spiking Neural Networks as a Controller for Emergent Swarm Agents.pdf;/home/amit/Zotero/storage/FZIBZKND/2410.html}
}

@misc{zhuSpikingNeuralNetworks2024a,
  title = {Spiking {{Neural Networks}} as a {{Controller}} for {{Emergent Swarm Agents}}},
  author = {Zhu, Kevin and Mattson, Connor and Snyder, Shay and Vega, Ricardo and Brown, Daniel S. and Parsa, Maryam and Nowzari, Cameron},
  year = {2024},
  month = oct,
  number = {arXiv:2410.16175},
  eprint = {2410.16175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.16175},
  urldate = {2025-05-02},
  abstract = {Drones which can swarm and loiter in a certain area cost hundreds of dollars, but mosquitos can do the same and are essentially worthless. To control swarms of low-cost robots, researchers may end up spending countless hours brainstorming robot configurations and policies to ``organically" create behaviors which do not need expensive sensors and perception. Existing research explores the possible emergent behaviors in swarms of robots with only a binary sensor and a simple but hand-picked controller structure. Even agents in this highly limited sensing, actuation, and computational capability class can exhibit relatively complex global behaviors such as aggregation, milling, and dispersal, but finding the local interaction rules that enable more collective behaviors remains a significant challenge. This paper investigates the feasibility of training spiking neural networks to find those local interaction rules that result in particular emergent behaviors. In this paper, we focus on simulating a specific milling behavior already known to be producible using very simple binary sensing and acting agents. To do this, we use evolutionary algorithms to evolve not only the parameters (the weights, biases, and delays) of a spiking neural network, but also its structure. To create a baseline, we also show an evolutionary search strategy over the parameters for the incumbent hand-picked binary controller structure. Our simulations show that spiking neural networks can be evolved in binary sensing agents to form a mill.},
  archiveprefix = {arXiv},
  keywords = {.toread,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,important,Neuromorphic processing,swarm},
  file = {/home/amit/Zotero/storage/TEKZDIED/Zhu et al. - 2024 - Spiking Neural Networks as a Controller for Emergent Swarm Agents.pdf;/home/amit/Zotero/storage/IXNHRGAQ/2410.html}
}
