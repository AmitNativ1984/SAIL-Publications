\documentclass{article}
\usepackage[preprint, nonatbib]{Styles/neurips_2025}
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

% \bibliographystyle{plainnat}

\title{Swarm Autonomy with Event-Driven Control and Spiking Neural Networks}

\author{
    Amit Nativ \\ 
    Supervisor: Oren Gal \\
    \\
    Swarm \& AI Lab \\
    Hatter Department of Marine Technologies \\
    Leon H. Charney School of Marine Sciences \\
    University of Haifa \\
    \\
    \today
}

\begin{document}

\maketitle

\begin{abstract}
    Quadrotors exhibit remarkable agility and speed when manually piloted through cluttered environments. However, achieving comparable autonomous capabilities under unknown conditions remains challenging, largely due to traditional visual sensor limitations and energy constraints. Biological sensing and processing are asynchronous and sparse, leading to low-latency and energy-efficient perception and action even in simple creatures such as small insects. Recently, event cameras, also known as neuromorphic cameras, have seen an increase in interest within the robotics community. Unlike traditional cameras, these bioinspired imaging sensors produce asynchronous, sparse visual data streams at very low latency, high dynamic range, and low energy consumption, making them well suited for high-speed flight. Spiking neural networks (SNNs) are a biologically inspired approach to neural networks, mirroring the functionality of the brain more closely than Artificial Neural Networks (ANNs). Their computational units, spiking neurons, allow for dynamic system representation, with spikes serving as the medium for asynchronous sparse communication among neurons. This research aims to explore the potential of combining event cameras and SNNs to develop end-to-end control policies for agile quadrotors. Additionally, the research will investigate emergent swarm behavior in multi-agent systems, where local SNN-based policies lead to complex collective behaviors without centralized control. This approach has the potential to significantly enhance the capabilities of single and multi-agent Unmanned Aerial Vehicles (UAVs) swarm systems, enabling them to navigate and operate effectively in cluttered and dynamic environments.
\end{abstract}

\section{Introduction}
Quadrotors can be flown very fast and with high agility by human pilots. Recent advances in autonomous, vision-based quadrotors have even shown champion-level performance in racing competitions \cite{kaufmannChampionlevelDroneRacing2023, romeroDreamFlyModelBased2025a, loquercioLearningHighSpeedFlight2021}. However, achieving high-speed vision-based autonomous flight through unknown environments remains challenging due to the inherent limitations of traditional cameras. Their limited frame rate, low dynamic range, and motion blur can increase uncertainty in traditional model-based mapping-planning-control pipelines or can cause out-of-distribution inputs for an end-to-end vision-to-control learning framework \cite{bhattacharyaVisionTransformersEndtoEnd2025}. 

Event-based vision represents a paradigm shift in visual sensing technology, inspired by biological visual systems capabilities (thus also referred to as neuromorphic vision) to detect and respond to changes in the environment. Unlike traditional frame cameras, which capture static images at set intervals, event-based vision utilizes neuromorphic cameras to continuously monitor light intensity changes at each pixel \cite{chakravarthiRecentEventCamera2024}. These cameras produce “events” only when significant changes occur, generating a dynamic data stream that reflects real-time scene dynamics. Event-based vision mimics the asynchronous nature of biological perception, where each pixel independently detects and records changes. This approach provides exceptionally high temporal resolution, essential for accurately capturing fast-moving objects and dynamic scenes without the motion blur typically associated with frame cameras. By focusing solely on changes and excluding static information, event cameras manage data more efficiently, significantly reducing redundancy and bandwidth requirements. The nearly continuous capture and processing of events enable immediate responses to scene changes, making event-based vision particularly suitable for high-speed motion and applications that require rapid decision-making \cite{DenseContinuousTimeOptical, gehrigERAFTDenseOptical2021, forraiEventbasedAgileObject2023, jiangFullyAsynchronousNeuromorphic2024, vidalUltimateSLAMCombining2018, falangaDynamicObstacleAvoidance2020}. The technology’s focus on detecting changes on a logarithmic scale rather than absolute values allows it to handle a wide range of lighting conditions effectively, avoiding common issues like overexposure or underexposure encountered with traditional systems \cite{gallegoEventbasedVisionSurvey2022, gehrigLowlatencyAutomotiveVision2024}. This adaptability is particularly valuable in environments with challenging lighting conditions in outdoor settings. Moreover, since event cameras process only the changes, they require less data bandwidth and computational power compared to traditional cameras. This efficiency results in significant energy savings, making event-based vision ideal for battery-powered devices. 

Spiking Neural Networks (SNNs) are a compelling natural match for event-based sensory data. They are often described as the third generation of Artificial Neural Networks (ANNs) \cite{maassNetworksSpikingNeurons1997a}, and have gained significant popularity in recent years \cite{gallegoEventbasedVisionSurvey2022}. Artificial neurons, such as Leaky-Integrate-and-Fire (LIF) or Adaptive Exponential, are computational primitives inspired by neurons found in the mammalian visual cortex, that form the basic building blocks of SNNs. A neuron receives input spikes ("events"), which modify its internal state (membrane potential) and produce an output spike (action potential) when the state surpasses a threshold \cite{gallegoEventbasedVisionSurvey2022}. As in biological neurons, which transmit information through short, discrete voltage spikes, SNNs encode data within the temporal dynamics of these spikes, each spike having the same magnitude. This event-driven operation sets SNNs apart from conventional ANNs, which rely on continuous numerical values for information representation. Consequently, the asynchronous, sparse and parallel processing characteristics inherent to SNNs result in substantially lower energy consumption, making them particularly suitable for hardware implementations \cite{farsaLowCostHighSpeedNeuromorphic2019, siddiqueLowCostNeuromorphic2023}. Additionally, the temporal dynamics of SNNs enhance their capability to model complex temporal patterns, offering greater expressiveness and adaptability in dynamic environments. Processing event-based visual streams with SNNs has been shown to be successful in various tasks, such as object detection \cite{iaboniEventbasedSpikingNeural2024}, optical flow estimation \cite{cuadradoOpticalFlowEstimation2023} and event-based stereo vision \cite{osswaldSpikingNeuralNetwork2017}. Despite these successes, applying SNNs to end-to-end control tasks from visual event streams in high speed and uncontrolled environemtns is still in its infancy \cite{stroobantsNeuromorphicAttitudeEstimation2025, paredes-vallesFullyNeuromorphicVision2023b, onizTrajectoryControlQuadrotors2024}, presenting an exciting research frontier. 

Emergent swarm intelligence refers to the spontaneous organization and adaptive collective behavior arising from local interactions among decentralized agents without centeral control. This phenomenon is observed in various biological systems, such as ant colonies, bird flocks, bee-hives, and fish schools, where simple rules followed by individual agents lead to complex group behaviors. In robotics, emergent swarm intelligence can be harnessed to create robust and flexible multi-agent robtic systems with a common goal, while adapting to dynamic environments. By leveraging local interactions and decentralized control, swarms can exhibit complex behaviors such as flocking, foraging, and exploration, which are essential for tasks like search and rescue, environmental monitoring, and autonomous navigation \cite{debieSwarmRoboticsSurvey2023}. SNNs are particularly compelling in this contex due to their biological inspiration and energy efficiency. In \cite{zhuSpikingNeuralNetworks2024}, SNNs were used to control milling behavior in a swarm of agents with simple binary binary sensing in a simulated environment. In \cite{zhaoNatureinspiredSelforganizingCollision2022}, SNNs were shown to control a swarm of quadrotors, enabling them to avoid intra-swarm collisions in a controlled setting. 

This research explores how end-to-end event-based vision and control with SNNs can be used to advance the autonomy of high-speed single and multi-agent robotic systems. Combining these technologies holds promise to enhance the autonomy, scalability, and robustness of both single-agent and swarm systems operating under energy constraints in challenging environments in the wild.

\section{Research Questions}
\begin{itemize}
    \item How can spiking neural networks be leverged to control agents using sparse, asynchronous event-based visual data?
    \item How can local SNN-based policies produce emergent swarm behavior?
    \item How can SNNs be optimized for neuromorphic hardware to improve energy efficiency and real-time performance?
\end{itemize}

\section{Research Objectives}
The research will revolve around the following objectives:    
\begin{itemize}
    \item \textbf{End-to-end control for agile UAVs flight in the wild with event cameras and SNNs:}
    Event-based cameras produce sparse, asynchronous visual input streams with high throughput, low latency and high dynamic range. Our goal is to leverage this sensor with SNNs, which are well suited for asynchronous sparse signals, and develop a novel end-to-end control framework that will lead to significant improvements in control, robustness and energy consumption of high-speed UAVs in the wild.
          
    \item \textbf{Develop decentralized navigation and control for UAV swarms in cluttered enviroments:} 
    Expand the single agent control policy to a swarm of agents, so that the swarm will exhibit emergent behavior. As each agent is controlled independently by a local SNN policy and receives local event-based visual input, we plan to improve the swarm's robustness to navigate through cluttered environments and avoid obstacles at high speeds.
            
    \item \textbf{Emergent swarm behavior with SNN based controllers:}
    Develop a framework for collective decision making and emergent behavior in swarms based on visual event based input and SNNs, such as flocking, foraging and exploration. The goal is to develop a framework that will allow the swarm to adapt to dynamic environments and exhibit complex behaviors without centralized control.
\end{itemize}

\section{Background}
\subsection{Vision Based Agile Flight in Cluttered Environments}
Various approaches have been studied in the litrature for vision based agile flight in cluttered environments. Particulary, in a traditional robotic design, The navigation task is divided into mapping, planning, and control. This line of work first requires computationally-intensive algorithms, such as Simultaneous Localization and Mapping (SLAM), to infer the 3D structure of the environment from 2D noisy image data \cite{zhouSwarmMicroFlying2022, scaramuzzaVisionControlledMicroFlying2014}. Given a 3D map of an environment, planning algorithms generate feasible trajectories that follow the shortest collision-free path from start to goal utilizing the vehicle’s full dynamic capabilities. Finally, a controller is used to follow the trajectory precisely, such as model predictive control \cite{falangaPAMPCPerceptionAwareModel2018}. Dividing the navigation task into a sequence of subtasks allows for simplifying the problem, resulting in an interpretable system from the engineering prespective. However, it leads to a pipeline that is sensitive to unmodeled effects due to a lack of interactions between each component. Also, the system requires additional latency for passing or waiting for the information \cite{loquercioLearningHighSpeedFlight2021}. Recently, learning-based methods attempted to address the aforementioned limitations. For instance, in \cite{loquercioLearningHighSpeedFlight2021} researchers propose to directly map noisy sensory observations to collisionfree trajectories in a receding-horizon fashion. This direct mapping forgoes the need for 3D environment mapping and collision waypoints planning. Though it reduces processing latency and increases robustness to noisy perception, a controller is still required to track the trajectory. Other works propose to learn end-to-end policies directly from sensory observations to control commands using imitation learning. Without relying on an expert, deep reinforcement learning has the potential to find more optimal policies for a variety of tasks. For aerial robots, Deep RL was successfully applied to high-speed flight in cluttered enviroments \cite{songLearningPerceptionAwareAgile2023}, drone racing \cite{kaufmannChampionlevelDroneRacing2023}. End-to-end vision-to-control learned policies show promising results in robustness, latency and speed, and surpass classical methods and panners 

\subsection{Event-Cameras}
\subsection{Spiking Neural Networks}
\subsection{Full Neuromorphic Control}
\subsection{Swarm Intelligence and Emergent Behavior}


\section{Methodology}

1. Flightmare simulator
2. MAVLab simulator
3. Event-based cameras
4. SNNtorch


\section{Novelty and Contribution}

\printbibliography[title={\section{References}}]

\end{document}