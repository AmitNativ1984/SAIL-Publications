\documentclass{article}
\usepackage[preprint, nonatbib]{Styles/neurips_2025}
\usepackage[backend=biber]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\addbibresource{references.bib}

% \bibliographystyle{plainnat}

\title{Swarm Autonomy with Event-Driven Control and Spiking Neural Networks}

\author{
    Amit Nativ \\ 
    Supervisor: Oren Gal \\
    \\
    Swarm \& AI Lab \\
    Hatter Department of Marine Technologies \\
    Leon H. Charney School of Marine Sciences \\
    University of Haifa \\
    \\
    \today
}

\begin{document}

\maketitle

\begin{abstract}
    Quadrotors exhibit remarkable agility and speed when manually piloted through cluttered environments. However, achieving comparable autonomous capabilities under unknown conditions remains challenging, largely due to traditional visual sensor limitations and energy constraints. Biological sensing and processing are asynchronous and sparse, leading to low-latency and energy-efficient perception and action even in simple creatures such as small insects. Recently, event cameras, also known as neuromorphic cameras, have seen an increase in interest within the robotics community. Unlike traditional cameras, these bioinspired imaging sensors produce asynchronous, sparse visual data streams at very low latency, high dynamic range, and low energy consumption, making them well suited for high-speed flight. Spiking neural networks (SNNs) are a biologically inspired approach to neural networks, mirroring the brain's functionality more closely than Artificial Neural Networks (ANNs). Their computational units, spiking neurons, allow for dynamic system representation, with spikes serving as the medium for asynchronous, sparse communication among neurons. This research aims to explore the potential of combining event cameras and SNNs to develop end-to-end control policies for agile quadrotors. Additionally, the research will investigate emergent swarm behavior in multi-agent systems, where local SNN-based policies lead to complex collective behaviors without centralized control. This approach has the potential to significantly enhance the capabilities of single and multi-agent Unmanned Aerial Vehicles (UAVs) swarm systems, enabling them to navigate and operate effectively in cluttered and dynamic environments.
\end{abstract}

\section{Introduction}
Quadrotors can be flown very fast and with high agility by human pilots. Recent advances in autonomous vision-based quadrotors have even shown champion-level performance in race competitions \cite{kaufmannChampionlevelDroneRacing2023, romeroDreamFlyModelBased2025a, loquercioLearningHighSpeedFlight2021}. However, achieving high-speed vision-based autonomous flight through unknown environments remains challenging due to the inherent limitations of traditional cameras. Their limited frame rate, low dynamic range, and motion blur can increase uncertainty in traditional model-based mapping-planning-control pipelines or can cause out-of-distribution inputs for an end-to-end vision-to-control learning framework \cite{bhattacharyaVisionTransformersEndtoEnd2025}. 

Event-based vision represents a paradigm shift in visual sensing technology, inspired by biological visual systems capabilities (thus also referred to as neuromorphic vision) to detect and respond to changes in the environment. Unlike traditional frame cameras, which capture static images at set intervals, event-based vision utilizes neuromorphic cameras to continuously monitor light intensity changes at each pixel \cite{chakravarthiRecentEventCamera2024}. These cameras produce “events” only when significant changes occur, generating a dynamic data stream that reflects real-time scene dynamics. Event-based vision mimics the asynchronous nature of biological perception, where each pixel independently detects and records changes. This approach provides exceptionally high temporal resolution, essential for accurately capturing fast-moving objects and dynamic scenes without the motion blur typically associated with frame cameras. By focusing solely on changes and excluding static information, event cameras manage data more efficiently, significantly reducing redundancy and bandwidth requirements. The nearly continuous capture and processing of events enable immediate responses to scene changes, making event-based vision particularly suitable for high-speed motion and applications that require rapid decision-making \cite{DenseContinuousTimeOptical, gehrigERAFTDenseOptical2021, forraiEventbasedAgileObject2023, jiangFullyAsynchronousNeuromorphic2024, vidalUltimateSLAMCombining2018, falangaDynamicObstacleAvoidance2020}. The technology’s focus on detecting changes on a logarithmic scale rather than absolute values allows it to handle a wide range of lighting conditions effectively, avoiding common issues like overexposure or underexposure encountered with traditional systems \cite{gallegoEventbasedVisionSurvey2022, gehrigLowlatencyAutomotiveVision2024}. This adaptability is particularly valuable in environments with challenging lighting conditions in outdoor settings. Moreover, since event cameras process only the changes, they require less data bandwidth and computational power compared to traditional cameras. This efficiency results in significant energy savings, making event-based vision ideal for battery-powered devices. 

Spiking Neural Networks (SNNs) are a compelling natural match for event-based sensory data. They are often described as the third generation of Artificial Neural Networks (ANNs) \cite{maassNetworksSpikingNeurons1997a}, and have gained significant popularity in recent years \cite{gallegoEventbasedVisionSurvey2022}. Artificial neurons, such as Leaky-Integrate-and-Fire (LIF) or Adaptive Exponential, are computational primitives inspired by neurons found in the mammalian visual cortex that form the basic building blocks of SNNs. A neuron receives input spikes ("events"), which modify its internal state (membrane potential) and produce an output spike (action potential) when the state surpasses a threshold \cite{gallegoEventbasedVisionSurvey2022}. As in biological neurons, which transmit information through short, discrete voltage spikes, SNNs encode data within the temporal dynamics of these spikes, each spike having the same magnitude. This event-driven operation sets SNNs apart from conventional ANNs, which rely on continuous numerical values for information representation. Consequently, the asynchronous, sparse, and parallel processing characteristics inherent to SNNs result in substantially lower energy consumption, making them particularly suitable for hardware implementations \cite{farsaLowCostHighSpeedNeuromorphic2019, siddiqueLowCostNeuromorphic2023}. Additionally, the temporal dynamics of SNNs enhance their capability to model complex temporal patterns, offering greater expressiveness and adaptability in dynamic environments. Processing event-based visual streams with SNNs has been shown to be successful in various tasks, such as object detection \cite{iaboniEventbasedSpikingNeural2024}, optical flow estimation \cite{cuadradoOpticalFlowEstimation2023}, and event-based stereo vision \cite{osswaldSpikingNeuralNetwork2017}. Despite these successes, applying SNNs to end-to-end control tasks from visual event streams in high-speed and uncontrolled environments is still in its infancy \cite{stroobantsNeuromorphicAttitudeEstimation2025, paredes-vallesFullyNeuromorphicVision2023b, onizTrajectoryControlQuadrotors2024}, presenting an exciting research frontier. 

Emergent swarm intelligence refers to the spontaneous organization and adaptive collective behavior arising from local interactions among decentralized agents without central control. This phenomenon is observed in various biological systems, such as ant colonies, bird flocks, bee-hives, and fish schools, where simple rules followed by individual agents lead to complex group behaviors. In robotics, emergent swarm intelligence can be harnessed to create robust and flexible multi-agent robotic systems with a common goal, while adapting to dynamic environments. By leveraging local interactions and decentralized control, swarms can exhibit complex behaviors such as flocking, foraging, and exploration, which are essential for tasks like search and rescue, environmental monitoring, and autonomous navigation \cite{debieSwarmRoboticsSurvey2023}. SNNs are particularly compelling in this context due to their biological inspiration and energy efficiency. In \cite{zhuSpikingNeuralNetworks2024}, SNNs were used to control milling behavior in a swarm of agents with simple binary sensing in a simulated environment. In \cite{zhaoNatureinspiredSelforganizingCollision2022}, SNNs were shown to control a swarm of quadrotors, enabling them to avoid intra-swarm collisions in a controlled setting. 

This research explores how end-to-end event-based vision and control with SNNs can be used to advance the autonomy of high-speed single and multi-agent robotic systems. Combining these technologies holds promise to enhance the autonomy, scalability, and robustness of both single-agent and swarm systems operating under energy constraints in challenging environments in the wild.

\section{Research Questions}
\begin{itemize}
    \item How can spiking neural networks be leveraged to control agents using sparse, asynchronous event-based visual data?
    \item How can local SNN-based policies produce emergent swarm behavior?
    \item How can events be effectively represented and processed by SNNs to enable high-speed navigation and control in cluttered environments?
    \item How can SNNs be optimized for neuromorphic hardware to improve energy efficiency and real-time performance?
\end{itemize}

\section{Research Objectives}
The research will revolve around the following objectives:    
\begin{itemize}
    \item \textbf{End-to-end control for agile UAVs flight in the wild with event cameras and SNNs:}
    Event-based cameras produce sparse, asynchronous visual input streams with high throughput, low latency, and high dynamic range. Our goal is to leverage this sensor with SNNs, which are well-suited for asynchronous sparse signals, and develop a novel end-to-end control framework that will lead to significant improvements in control, robustness, and energy consumption of high-speed UAVs in the wild.
          
    \item \textbf{Develop decentralized navigation and control for UAV swarms in cluttered environments :} 
    Expand the single-agent control policy to a swarm of agents, so that the swarm will exhibit emergent behavior. As each agent is controlled independently by a local SNN policy and receives local event-based visual input, we plan to improve the swarm's robustness to navigate through cluttered environments and avoid obstacles at high speeds.
            
    \item \textbf{Emergent swarm behavior with SNN-based controllers:}
    Develop a framework for collective decision making and emergent behavior in swarms based on visual event-based input and SNNs, such as flocking, foraging, and exploration. The goal is to develop a framework that will allow the swarm to adapt to dynamic environments and exhibit complex behaviors without centralized control.
\end{itemize}

\section{Background}
\subsection{Vision Based Agile Flight in Cluttered Environments}
Various approaches have been studied in the literature for vision-based agile flight in cluttered environments. In a traditional robotic design, the navigation task is divided into mapping, planning, and control. This line of work first requires computationally-intensive algorithms, such as Simultaneous Localization and Mapping (SLAM), to infer the 3D structure of the environment from 2D noisy image data \cite{zhouSwarmMicroFlying2022, scaramuzzaVisionControlledMicroFlying2014}. Given a 3D map of an environment, planning algorithms generate feasible trajectories that follow the shortest collision-free path from start to goal, utilizing the vehicle’s full dynamic capabilities. Finally, a controller is used to follow the trajectory precisely, such as model predictive control \cite{falangaPAMPCPerceptionAwareModel2018}. Dividing the navigation task into a sequence of subtasks allows for simplifying the problem, resulting in an interpretable system from the engineering perspective. However, it leads to a pipeline that is sensitive to unmodeled effects due to a lack of interactions between each component. Also, the system requires additional latency for passing or waiting for the information \cite{loquercioLearningHighSpeedFlight2021}. Learning-based methods attempt to address the aforementioned limitations. For instance, in \cite{loquercioLearningHighSpeedFlight2021}, researchers propose to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping forgoes the need for 3D environment mapping and collision waypoint planning. This method reduces processing latency and increases robustness and success rates of speeds up to 10m/s, but a controller is still required to track the trajectory. Other works propose to learn end-to-end policies directly from sensory observations to control commands using imitation learning and deep reinforcement learning (RL). Without relying on an expert, RL has the potential to find more optimal policies for a variety of tasks. For aerial robots, deep RL was successfully applied to high-speed flight in cluttered environments \cite{songLearningPerceptionAwareAgile2023} and drone racing \cite{kaufmannChampionlevelDroneRacing2023}, winning a race against world-champion human pilots. End-to-end vision-to-control learned policies show promising results in robustness, latency, and speed, and surpass classical methods and planners.

\subsection{Event-Cameras and Event-Based Vision}
Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a data rate sequence of digital "events" or "spikes", with each event representing a change of brightness (on logarithmic scale) of predefined magnitude at a specific pixel location and time. Each pixel memorizes the log intensity each time it sends an event, and continuously monitors for a change of sufficient magnitude from their memorized value. When the change exceeds a certain threshold, the camera sends an event. Each event is represented as a tuple $e_k(\textbf{x}_k,t_k,p_k)$,
where $\textbf{x}_k$ represents the pixel coordinates, the indepent timestamp of the event $t_k$, and the 1-bit polarity of the change (i.e., brightness increase "ON" or decrease "OFF") $p_k$. Compared to traditional cameras, event cameras offer several advantages: \textit{Low latency:} Each pixel operates asynchronously with no need for global exposure time, allowing for sub-millisecond latency in capturing changes, while traditional cameras have tens of milliseconds of latency. \textit{Low power consumption:} Event cameras transmit only changes in brightness, significantly reducing redundant data and power usage, typically around 10 mW vs. 1W. \textit{High dynamic range:} Operating on a logarithmic scale, event cameras can handle a wide range of lighting conditions, achieving dynamic ranges greater than 120 dB compared to 60 dB for standard cameras. 

However, this paradigm shift is not without challenges, especially: \textit{Coping with different space-time output:} The output of event cameras is fundamentally different from that of standard cameras: events are asynchronous and spatially sparse, whereas images are synchronous and dense. For this reason, frame-based vision algorithms designed for image sequences are not directly applicable to event data. \textit{Coping with different photometric sensing:} In contrast to the grayscale information that standard cameras provide, each event contains binary (increase/decrease) brightness change information and no intensity information. Brightness changes depend not only on the scene brightness, but also on the current and past relative motion between the scene and the camera. \textit{Lack of data:} Because event cameras are relatively new, event data is much more rare than image data, and there is a lack of large-scale datasets for training and evaluation. 

\subsubsection{Event Generation Model}
An event is triggered at a single pixel as soon as the magnitude of brightness changes on a logarithmic scale reaches a threshold. Neglecting sensor noise, we can model the event generation process as follows \cite{gallegoEventbasedVisionSurvey2022}:
\begin{equation}
    \log(I(\textbf{x}_k,t_k)) - \log(I(\textbf{x}_k,t_k-\Delta t_k)) = \pm C,
    \label{eq:event_generation_model}
\end{equation}
    
Where $C>0$ is the camera threshold that can be adjusted to control the sensitivity of the camera. $\Delta t_k$ is the time since the last event. From Eq.~\eqref{eq:event_generation_model}, and following the rule of integration, it is possible to reconstruct the \textit{relative} intensity of the scene from the stream of events. The reconstructed image does not suffer from motion blur and has a very high dynamic range \cite{rebecqHighSpeedHigh2019}. 

However, reconstructing the image from events adds latency and requires additional processing. Instead, it is often more efficient to process the events directly without reconstructing the image, especially in high-speed scenarios where latency is critical. Two schools of approaches can be distinguished, and the choice of method depends on the specific application and the required latency:

\subparagraph{Event-by-event methods:} These methods process each event as it arrives, allowing for immediate responses to changes in the scene. They are suitable for real-time applications where very low latency is crucial, such as in high-speed motion and dynamic environments. Following Eq.~\eqref{eq:event_generation_model}, events are generated by either temporal brightness changes or by moving edges in the scene (assuming constant illumination and brightness) caused by the relative motion of the camera with respect to the scene. Let $L(x,y,t)=\log(I(x,y,t))$, and an intensity gradient (an edge) $\nabla L(\textbf{x}_k,t_k)$, undergoing a relative motion $\textbf{u}$ with respect to the camera, the event generation model for a small $\Delta t$ can be expressed as follows \cite{gallegoEventbasedVisionSurvey2022, gallegoEventbasedCameraPose2015}:
\begin{equation}
    -\nabla L(\textbf{x}_k,t_k) \cdot \textbf{u}\Delta t = \pm C
    \label{eq:moving_edges_event}
\end{equation}
Event-by-event methods have been proven successful in tasks such as optical flow and simultaneous localization and mapping (SLAM) under extreme conditions \cite{vidalUltimateSLAMCombining2018, cuadradoOpticalFlowEstimation2023,gehrigERAFTDenseOptical2021, sunAutonomousQuadrotorFlight2021}

\subparagraph{Batch methods:} These methods process typically involve accumulating events over a time window or a fixed number of events, and then processing them to extract information or estimate parameters. These methods offer better performance in noisy environments, at the cost of increased latency (still much lower than traditional cameras). The spatio-temporal volume of events in the batch depends on the scene and its dynamics relative to the camera, but it is impossible to understand the underlying scene simply by looking at the event cloud. Contrast maximization methods wrap the spatiotemporal volume of events by solving an optimization problem to maximize the contrast of the events. The objective function is to maximize the focus of the resulting image (max the variance) called Image of Wrapped Events (IWE). This is done by iterating through the parameters of the motion dynamics with gradient-based methods, until a certain maximum is reached \cite{gallegoUnifyingContrastMaximization2018a, gallegoEventbasedVisionSurvey2022, gallegoFocusAllYou2019}. This method has been successfully applied to problems such as rotational motion estimation and compensation \cite{gallegoAccurateAngularVelocity2017,xingEROAMEventbasedCamera2024}, motion segmentation \cite{stoffregenEventBasedMotionSegmentation2019}. Contrast maximization problems have also been successfully applied to rapid decision making in robotics. For instance, by separating the motion caused by the ego-motion of the drone and an obstacle, a drone was able to dodge an obstacle with relative speeds of up to 10 m/s, with compute latency of 3.5 m/s simply on a CPU. In \cite{forraiEventbasedAgileObject2023}, similar principles were used to catch a fast-moving object with a quadropad.

\subsubsection{Learning with Event Cameras} Recent advancements in artificial neural networks have paved the way for many successful applications of standard cameras. However, the sparse and asynchronous nature of event cameras presents unique challenges to ANNs, which are typically designed for synchronous and dense data. To address this, three main approaches have emerged.

\subparagraph{Synchronous, Dense ANNs} In this type of approach, the continuous stream of events is converted into a 3D voxel grid $(x,y,t)$ representation: the value of each voxel is binary, according the polarity of the accumetlated events within the voxel during the time window. This representation is similar to 2D images at high frame rates, with binary values. This method is used for image deblurring and upsampling low frame rate video to a very high frame rate with only a small memory footprint \cite{tulyakovTimeLensEventbasedVideo2021}. This is a very common approach to process events with ANNs, but it is not ideal in the event-based paradigm because it quantizes event timestamps. 

\subparagraph{Asynchronous, Sparse Graph Neural Networks} Graph neural networks (GNNs) are a class of neural networks that operate on graph-structured data. GNNs respect the sparse and asynchronous nature of event data by treating events as nodes in a graph. Each event is then connected to its neighboring events within a certain radius. This approach allows for the processing of events in a way that respects their sparse temporal and spatial relationships, and was proven successful in tasks such as object classification and action recognition \cite{biGraphBasedSpatioTemporalFeature2020, dengVoxelGraphCNN2021}

\subparagraph{Asynchronous, SNNs} Spiking neural networks (SNNs) are inherently suited for processing asynchronous and sparse data, making them an ideal match for event-based vision. By integrating an event camera with an SNN deployed on a neuromorphic chip, such as Intel's Loihi, robots can process visual information at the sensor's native speed, enabling near-instant reactions to changes while consuming significantly less energy compared to frame-based systems. SNNs have demonstrated remarkable efficiency in handling event streams: for instance, an SNN-based optical flow estimation for driving scenarios, utilizing a U-Net architecture on event data, successfully produced dense flow maps after training with surrogate gradients \textbf{frontiersin.org}. Similarly, spiking networks have been applied to event-based segmentation tasks, with models like SpikeSEG and SpikeMS processing DVS inputs to segment scenes or detect motion directly from spike streams rather than images \textbf{pmc.ncbi.nlm.nih.gov, pmc.ncbi.nlm.nih.gov}. These methods achieve accuracy comparable to frame-based deep networks while consuming significantly less energy—often tens of times less \textbf{pmc.ncbi.nlm.nih.gov}. Such energy efficiency is crucial for mobile robots and drones operating with limited power resources. Although challenges persist in scaling and training these networks, recent advancements have been substantial. SNNs are establishing themselves as a promising solution in scenarios requiring rapid, efficient, and event-driven intelligence. In the coming years, spiking neural networks are expected to evolve from experimental systems to essential components of robotic platforms, leveraging their strengths in temporal processing and energy-efficient operation.

---

\subsection{Spiking Neural Networks}
\subsection{Full Neuromorphic Control}
\subsection{Swarm Intelligence and Emergent Behavior}

\section{Methodology}

1. Flightmare simulator
2. MAVLab simulator
3. Event-based cameras
4. SNNtorch


\section{Novelty and Contribution}

\printbibliography[title={\section{References}}]

\end{document}