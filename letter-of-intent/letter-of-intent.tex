\documentclass{article}
\usepackage[preprint, nonatbib]{Styles/neurips_2025}
\usepackage[backend=biber,
            sorting=none, 
            style=numeric-comp,
            url=false,
            doi=false,
            isbn=false
            ]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}

\addbibresource{references.bib}

\title{Swarm Autonomy with Event-Driven Control and Spiking Neural Networks}

\author{
    Amit Nativ \\ 
    Supervisor: Oren Gal \\
    \\
    Swarm \& AI Lab \\
    Hatter Department of Marine Technologies \\
    Leon H. Chaney School of Marine Sciences \\
    University of Haifa \\
    \\
    \today
}

\begin{document}

\maketitle

\begin{abstract}
    Quadrotors exhibit remarkable agility and speed when manually piloted through cluttered environments. However, achieving comparable autonomous capabilities under unknown conditions remains challenging, largely due to traditional visual sensor limitations and energy constraints. Biological sensing and processing are asynchronous and sparse, leading to low-latency and energy-efficient perception and action even in simple creatures such as small insects. Recently, event cameras, also known as neuromorphic cameras, have seen an increase in interest within the robotics community. Unlike traditional cameras, these bio-inspired imaging sensors produce asynchronous, sparse visual data streams at very low latency, high dynamic range, and low energy consumption, making them well suited for high-speed flight. Spiking neural networks (SNNs) are a biologically inspired approach to neural networks, mirroring the brain's functionality more closely than Artificial Neural Networks (ANNs). Their computational units, spiking neurons, allow for dynamic system representation, with spikes serving as the medium for asynchronous, sparse communication among neurons. This research explores the potential of combining event cameras and SNNs to develop end-to-end control policies for agile quadrotors. Additionally, the research will investigate emergent swarm behavior in multi-agent systems, where local SNN-based policies lead to complex collective behaviors without centralized control. This approach has the potential to significantly enhance the capabilities of single and multi-agent Unmanned Aerial Vehicles (UAVs) swarm systems, enabling them to navigate at high speeds and operate effectively in cluttered and dynamic environments, while improving their energy consumption. 
\end{abstract}

\section{Introduction}
Quadrotors can be flown very fast and with high agility by human pilots. Recent advances in autonomous vision-based quadrotors have even shown champion-level performance in race competitions \cite{kaufmannChampionlevelDroneRacing2023, romeroDreamFlyModelBased2025a, loquercioLearningHighSpeedFlight2021}. However, achieving autonomous high-speed vision flight through unknown environments remains challenging due to the inherent limitations of traditional cameras. Their limited frame rate, low dynamic range, and motion blur can increase uncertainty in traditional model-based mapping-planning-control pipelines or can cause out-of-distribution inputs for an end-to-end vision-to-control learning framework \cite{bhattacharyaVisionTransformersEndtoEnd2025}. 

Event-based vision represents a paradigm shift in visual sensing technology, inspired by the capabilities of biological visual systems (thus also referred to as neuromorphic vision) to detect and respond to changes in the environment. Unlike traditional frame-based cameras, which capture static images at set intervals, event-based cameras mimic the asynchronous nature of biological perception, continuously monitoring changes in light intensity in each pixel independently \cite{chakravarthiRecentEventCamera2024}. These cameras produce \textit{events} only when significant changes occur on a logarithmic scale, generating a dynamic asynchronous data stream at exceptionally high temporal resolution. The nearly continuous capture of events enables immediate response to scene changes, making event-based vision particularly suitable for high-speed motion and applications that require rapid decision making without the motion blur typically associated with frame cameras \cite{DenseContinuousTimeOptical, gehrigERAFTDenseOptical2021, forraiEventbasedAgileObject2023, jiangFullyAsynchronousNeuromorphic2024, vidalUltimateSLAMCombining2018, falangaDynamicObstacleAvoidance2020}. The technologyâ€™s focus on detecting changes on a logarithmic scale rather than absolute values allows it to handle a wide range of lighting conditions effectively, avoiding common issues like overexposure or underexposure encountered with traditional systems \cite{gallegoEventbasedVisionSurvey2022, gehrigLowlatencyAutomotiveVision2024}. This adaptability is particularly valuable in environments with challenging lighting conditions in outdoor settings. Moreover, since event cameras only process changes, they require less data bandwidth and computational power compared to traditional cameras. This efficiency results in significant energy savings, making event-based vision ideal for battery-powered devices. 

Spiking Neural Networks (SNNs) are a compelling natural match for event-based sensory data. They are often described as the third generation of Artificial Neural Networks (ANNs) \cite{maassNetworksSpikingNeurons1997a}, and have gained significant popularity in recent years \cite{gallegoEventbasedVisionSurvey2022}. Artificial neurons, such as Leaky-Integrate-and-Fire (LIF) or Adaptive Exponential, are computational primitives inspired by neurons found in the mammalian visual cortex that form the basic building blocks of SNNs. A neuron receives input spikes (events), which modify its internal state (membrane potential) and produce an output spike (action potential) when the state exceeds a threshold \cite{gallegoEventbasedVisionSurvey2022}. As in biological neurons, which transmit information through short, discrete voltage spikes, SNNs encode data within the temporal dynamics of these spikes, each spike having the same magnitude. This event-driven operation sets SNNs apart from conventional ANNs, which rely on continuous numerical values for information representation. Consequently, the asynchronous, sparse, and parallel processing characteristics inherent in SNNs result in substantially lower energy consumption, making them particularly suitable for hardware implementations under power constraints \cite{farsaLowCostHighSpeedNeuromorphic2019, siddiqueLowCostNeuromorphic2023}. Additionally, the temporal dynamics of SNNs enhance their ability to model complex temporal patterns, offering greater expressiveness and adaptability in dynamic environments. The processing of event-based visual streams with SNNs has been shown to be successful in various tasks, such as object detection \cite{iaboniEventbasedSpikingNeural2024}, optical flow estimation \cite{cuadradoOpticalFlowEstimation2023}, and event-based stereo vision \cite{osswaldSpikingNeuralNetwork2017}. Despite these successes, applying SNNs to end-to-end control tasks from visual event streams in high-speed and uncontrolled environments is still in its infancy \cite{stroobantsNeuromorphicAttitudeEstimation2025, paredes-vallesFullyNeuromorphicVision2023b, onizTrajectoryControlQuadrotors2024}, presenting an exciting research frontier. 

Emergent swarm intelligence refers to the spontaneous organization and adaptive collective behavior arising from local interactions among decentralized agents without central control. This phenomenon is observed in various biological systems, such as ant colonies, bird flocks, bee-hives, and fish schools, where simple rules followed by individual agents lead to complex group behaviors. In robotics, emergent swarm intelligence can be harnessed to create robust and flexible multi-agent robotic systems with a common goal, while adapting to dynamic environments. By leveraging local interactions and decentralized control, swarms can exhibit complex behaviors such as flocking, foraging, and exploration, which are essential for tasks like search and rescue, environmental monitoring, and autonomous navigation \cite{debieSwarmRoboticsSurvey2023}. SNNs are particularly compelling in this context due to their biological inspiration and energy efficiency. In \cite{zhuSpikingNeuralNetworks2024}, SNNs were used to control milling behavior in a swarm of agents with simple binary sensing in a simulated environment. In \cite{zhaoNatureinspiredSelforganizingCollision2022}, SNNs were shown to control a swarm of quadrotors, enabling them to avoid intra-swarm collisions in a controlled setting. However, applying SNNs to control swarms from event-based visual streams in uncontrolled environments has not been largely explored yet. 

This research explores how end-to-end event-based vision and control with SNNs can be used to advance the autonomy of high-speed single and multi-agent robotic systems. Combining these technologies holds promise to enhance the autonomy, robustness and mission duration of both single-agent and swarm systems operating under energy constraints in challenging environments in the wild.

\section{Research Questions}
\begin{itemize}
    \item How can spiking neural networks be leveraged to control agents using sparse, asynchronous event-based visual data?
    \item How can events be effectively represented and processed by SNNs to enable high-speed navigation and control in cluttered environments?
    \item How can local SNN-based policies produce emergent swarm behavior?
    \item How can SNNs be optimized for neuromorphic hardware to improve energy efficiency and real-time performance?
\end{itemize}

\section{Research Objectives}
The research will revolve around the following objectives:    
\begin{itemize}
    \item \textbf{End-to-end control for agile UAVs flight in the wild with event cameras and SNNs:}
    Event-based cameras produce sparse, asynchronous visual input streams with high throughput, low latency, and high dynamic range. Our goal is to leverage this sensor with SNNs, which are well-suited for asynchronous sparse signals, and develop a novel end-to-end control framework that will lead to significant improvements in control, robustness, and energy consumption of high-speed UAVs in the wild.
          
    \item \textbf{Efficient decentralized navigation of UAV swarms in cluttered environments:} By expanding the single-agent SNN control policy to a swarm of agents, we plan to improve the swarm's robustness to collectively navigate through cluttered environments and avoid obstacles at high speeds.
            
    \item \textbf{Emergent swarm behavior with SNN-based controllers:}
    Develop a framework for collective decision making and emergent behavior in swarms based on visual event-based input and SNNs, such as flocking, foraging, and exploration. The goal is to develop a framework that will allow the swarm to adapt to dynamic environments and exhibit complex behaviors without centralized control.
\end{itemize}

\section{Background}
\subsection{Vision Based Agile Flight in Cluttered Environments}
Various approaches have been studied in the literature for vision-based agile flight in cluttered environments. In a traditional robotic design, the navigation task is divided into mapping, planning, and control. This line of work first requires computationally-intensive algorithms, such as Simultaneous Localization and Mapping (SLAM), to infer the 3D structure of the environment from 2D noisy image data \cite{zhouSwarmMicroFlying2022, scaramuzzaVisionControlledMicroFlying2014}. Given a 3D map of an environment, planning algorithms generate feasible trajectories that follow the shortest collision-free path from start to goal, utilizing the vehicleâ€™s full dynamic capabilities. Finally, a controller is used to follow the trajectory precisely, such as model predictive control \cite{falangaPAMPCPerceptionAwareModel2018}. Dividing the navigation task into a sequence of subtasks allows for simplifying the problem, resulting in an interpretable system from the engineering perspective. However, it leads to a pipeline that is sensitive to unmodeled effects due to a lack of interactions between each component. Also, the system requires additional latency for passing or waiting for the information \cite{loquercioLearningHighSpeedFlight2021}. Learning-based methods attempt to address the aforementioned limitations. For instance, in \cite{loquercioLearningHighSpeedFlight2021}, researchers propose to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping forgoes the need for 3D environment mapping and collision waypoint planning. This method reduces processing latency and increases robustness and success rates of speeds up to 10m/s, but a controller is still required to track the trajectory. Other works propose to learn end-to-end policies directly from sensory observations to control commands using imitation learning and deep reinforcement learning (RL). Without relying on an expert, RL has the potential to find more optimal policies for a variety of tasks. For aerial robots, deep RL was successfully applied to high-speed flight in cluttered environments \cite{songLearningPerceptionAwareAgile2023} and drone racing \cite{kaufmannChampionlevelDroneRacing2023}, winning a race against world-champion human pilots. End-to-end vision-to-control learned policies show promising results in robustness, latency, and speed, and surpass classical methods and planners.

\subsection{Event-Cameras and Event-Based Vision}
Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a data rate sequence of digital "events" or "spikes", with each event representing a change of brightness (on logarithmic scale) of predefined magnitude at a specific pixel location and time. Each pixel memorizes the log intensity each time it sends an event, and continuously monitors for a change of sufficient magnitude from its memorized value. When the change exceeds a certain threshold, the camera sends an event. Each event is represented as a tuple $e_k(\textbf{x}_k,t_k,p_k)$,
where $\textbf{x}_k$ represents the pixel coordinates, the independent timestamp of the event $t_k$, and the 1-bit polarity of the change (i.e., brightness increase "ON" or decrease "OFF") $p_k$. Compared to traditional cameras, event cameras offer several advantages: \textit{Low latency:} Each pixel operates asynchronously with no need for global exposure time, allowing for sub-millisecond latency in capturing changes, while traditional cameras have tens of milliseconds of latency. \textit{Low power consumption:} Event cameras transmit only changes in brightness, significantly reducing redundant data and power usage, typically around 10 mW vs. 1W. \textit{High dynamic range:} Operating on a logarithmic scale, event cameras can handle a wide range of lighting conditions, achieving dynamic ranges greater than 120 dB compared to 60 dB for standard cameras. 

However, this paradigm shift is not without challenges, especially: \textit{Coping with different space-time output:} The output of event cameras is fundamentally different from that of standard cameras: events are asynchronous and spatially sparse, whereas images are synchronous and dense. For this reason, frame-based vision algorithms designed for image sequences are not directly applicable to event data. \textit{Coping with different photometric sensing:} In contrast to the grayscale information that standard cameras provide, each event contains binary (increase/decrease) brightness change information and no intensity information. Brightness changes depend not only on the scene brightness, but also on the current and past relative motion between the scene and the camera. \textit{Lack of data:} Because event cameras are relatively new, event data is much more scarce than image data, and there is a lack of large-scale datasets for training and evaluation. 

\subsubsection{Event Generation Model}\label{sec:event_generation_model}
An event is triggered at a single pixel as soon as the magnitude of brightness changes on a logarithmic scale reaches a threshold. Neglecting sensor noise, we can model the event generation process as follows \cite{gallegoEventbasedVisionSurvey2022}:

\begin{equation}\label{eq:event_generation_model}
    \log(I(\textbf{x}_k,t_k)) - \log(I(\textbf{x}_k,t_k-\Delta t_k)) = \pm C
\end{equation}
    
Where $C>0$ is the camera threshold that can be adjusted to control the sensitivity of the camera. $\Delta t_k$ is the time since the last event. From Eq.~\eqref{eq:event_generation_model}, and following the rule of integration, it is possible to reconstruct the \textit{relative} intensity of the scene from the stream of events. The reconstructed image does not suffer from motion blur and has a very high dynamic range \cite{rebecqHighSpeedHigh2019}. 

However, reconstructing the image from events adds latency and requires additional processing. Instead, it is often more efficient to process the events directly without reconstructing the image, especially in high-speed scenarios where latency is critical. Two schools of approaches can be distinguished, and the choice of method depends on the specific application and the required latency:

\subparagraph{Event-by-event methods:} These methods process each event as it arrives, allowing for immediate responses to changes in the scene. They are suitable for real-time applications where very low latency is crucial, such as in high-speed motion and dynamic environments. Following Eq.~\eqref{eq:event_generation_model}, events are generated by either temporal brightness changes or by moving edges in the scene (assuming constant illumination and brightness) caused by the relative motion of the camera with respect to the scene. Let $L(x,y,t)=\log(I(x,y,t))$, and an intensity gradient (an edge) $\nabla L(\textbf{x}_k,t_k)$, undergoing a relative motion $\textbf{u}$ with respect to the camera, the event generation model for a small $\Delta t$ can be expressed as follows \cite{gallegoEventbasedVisionSurvey2022, gallegoEventbasedCameraPose2015}:

\begin{equation}\label{eq:moving_edges_event}
    -\nabla L(\textbf{x}_k,t_k) \cdot \textbf{u}\Delta t = \pm C    
\end{equation}

Event-by-event methods have been proven successful in tasks such as optical flow and simultaneous localization and mapping (SLAM) under extreme conditions \cite{vidalUltimateSLAMCombining2018, cuadradoOpticalFlowEstimation2023,gehrigERAFTDenseOptical2021, sunAutonomousQuadrotorFlight2021}

\subparagraph{Batch methods:} These methods typically involve accumulating events over a time window or a fixed number of events, and then processing them to extract information or estimate parameters. These methods offer better performance in noisy environments, at the cost of increased latency (still much lower than traditional cameras). The spatio-temporal volume of events in the batch depends on the scene and its dynamics relative to the camera, but it is impossible to understand the underlying scene simply by looking at the event cloud. Contrast maximization methods wrap the spatiotemporal volume of events by solving an optimization problem to maximize the contrast of the events. The objective function is to maximize the focus of the resulting image (i.e., max the variance) called Image of Wrapped Events (IWE). This is done by iterating through the parameters of the motion dynamics with gradient-based methods, until a certain maximum is reached \cite{gallegoUnifyingContrastMaximization2018a, gallegoEventbasedVisionSurvey2022, gallegoFocusAllYou2019}. This method has been successfully applied to problems such as rotational motion estimation and compensation \cite{gallegoAccurateAngularVelocity2017,xingEROAMEventbasedCamera2024}, motion segmentation \cite{stoffregenEventBasedMotionSegmentation2019}. Contrast maximization problems have also been successfully applied to rapid decision making in robotics. For instance, by separating the motion caused by the ego-motion of the drone and an obstacle, a drone was able to dodge an obstacle with relative speeds of up to 10 m/s, with compute latency of 3.5 m/s simply on a CPU. In \cite{forraiEventbasedAgileObject2023}, similar principles were used to catch a fast-moving object with a quadroped.

\subsubsection{Learning with Event Cameras} Recent advancements in artificial neural networks have paved the way for many successful applications of standard cameras. However, the sparse and asynchronous nature of event cameras presents unique challenges to ANNs, which are typically designed for synchronous and dense data. To address this, three main approaches have emerged.

\subparagraph{Synchronous, Dense ANNs} In this type of approach, the continuous stream of events is converted into a 3D voxel grid $(x,y,t)$ representation: the value of each voxel is binary, according the polarity of the accumulated events within the voxel during the time window. This representation is similar to 2D images at high frame rates, with binary values. This method is used for image deblurring and upsampling low frame rate video to a very high frame rate with only a small memory footprint \cite{tulyakovTimeLensEventbasedVideo2021}. This is a very common approach to process events with ANNs, but it is not ideal in the event-based paradigm because it quantizes event timestamps. 

\subparagraph{Asynchronous, Sparse Graph Neural Networks} Graph neural networks (GNNs) are a class of neural networks that operate on graph-structured data. GNNs respect the sparse and asynchronous nature of event data by treating events as nodes in a graph. Each event is then connected to its neighboring events within a certain radius. This approach allows for the processing of events in a way that respects their sparse temporal and spatial relationships, and was proven successful in tasks such as object classification and action recognition \cite{biGraphBasedSpatioTemporalFeature2020, dengVoxelGraphCNN2021}

\subparagraph{Asynchronous, SNNs} Spiking neural networks (SNNs) are inherently suited for processing asynchronous and sparse data, making them an ideal match for event-based vision. By integrating an event camera with an SNN deployed on a neuromorphic chip, such as Intel's Loihi, robots can process visual information at the sensor's native speed, enabling near-instant reactions to changes while consuming significantly less energy compared to frame-based systems. SNNs have demonstrated remarkable efficiency in handling event streams: for instance, an SNN-based optical flow estimation for driving scenarios, utilizing a U-Net architecture on event data, successfully produced dense flow maps after training with surrogate gradients \cite{cuadradoOpticalFlowEstimation2023}. Similarly, spiking networks have been applied to event-based segmentation tasks, with models like SpikeMS processing visual event stream to segment scenes or detect motion directly from spike streams rather than images \cite{parameshwaraSpikeMSDeepSpiking2021}. These methods achieve accuracy comparable to frame-based deep networks while consuming significantly less energyâ€”often tens of times less \cite{yamazakiSpikingNeuralNetworks2022}. Such energy efficiency is crucial for mobile robots and drones operating with limited power resources. Although challenges persist in scaling and training these networks, recent advancements have been substantial. SNNs are establishing themselves as a promising solution in scenarios requiring rapid, efficient, and event-driven intelligence. In the coming years, spiking neural networks are expected to evolve from experimental systems to essential components of robotic platforms, leveraging their strengths in temporal processing and energy-efficient operation.

\subsection{Spiking Neural Networks}
Unlike standard ANNs that use continuous-valued activation. SNN use biologically realistic neuron models that bridge the gap between neuroscience and machine learning. Different neuron models have been proposed, among which the \textit{Leaky Integrate-and-Fire (LIF)} model is the most widely used due to its simplicity and effectiveness. In this model, the neuron membrane potential gradually decays (leaks) over time while accumulating incoming spikes until a thersold is reaches, upon which a spike is fired and voltage resets. It is modeled by a first-order differential equation that describes the dynamics of the membrane potential. The LIF model captures temporal dynamics and refractory periods,  while remaining computationally efficient \cite{yamazakiSpikingNeuralNetworks2022, izhikevichSimpleModelSpiking2003, eshraghianTrainingSpikingNeural2023}. This makes SNNs naturally fit to analyse temporal data.

\subsubsection{Spike Encoding}
In ANNs, information is typically encoded in the magnitude of the input and output signals of neurons. However, in SNNs, all spikes are identical with the same magnitude, so information cannot be encoded in the magnitude of the input and output spikes. Two main coding schemes are used to encode information in SNNs \cite{eshraghianTrainingSpikingNeural2023}:

\paragraph {Rate Coding} converts signals into spike trains, where the \textit{rate} or \textit{spike count} within a specific time window represents the information. As a rudimentaty example, a bright pixel is encoded into a high frame rate, wheres a dark pixel would result in low frequency firing. From an output perspective, the neuron with the highest firing rate within the time window represents the most salient feature in the input. In a classification task, the neuron with the highest firing rate can be interpreted as the predicted class. Compared to temporal coding, rate coding is considered easier to learn (more spiking promotes more learning) with a higher error tolerance (if a neuron fails to fire, many more spikes can still generate a meaningful signal).

\paragraph{Temporal (Latency) Coding} encodes information in the precise timimg of a spike. The total number of spikes is no longer consequiential. Rather, \textit{when} the spike occurs is what matters. When compared to rate coding, temporal coding assisgns much more meanininng to each individual spike, resulting in a more efficient representation of information in terms of bandwidth and energy consumption. For example, in a classification task, the neuron that spikes first represents the predicted class. This is particularly useful in scenarios where timing is crucial, such as in robotics and neuromorphic computing.

\subsubsection {Training Methodologies for SNNs}
Training SNNs remains a central challenge because the spike activation functions are non-differentiable (zero gradients almost everywhere and undefined at spike times), making standard backpropagation difficult. Broadly, three learning paradigms have been explored in SNN research:

\paragraph{Surrogate Gradient Descent (Spike-Based Backpropagation):} This approach approximates the gradient of the spike activation as a continuous differentiable function, such as the derivative of a sigmoid, but only during backward pass. This essentialliy provides a continous differential function allowing for the use of standard backpropagation methods as in ANNS \cite{yamazakiSpikingNeuralNetworks2022, eshraghianTrainingSpikingNeural2023}.

\paragraph{Spike-Time-Dependent Plasticity (STDP):} An unsupervised learning rule that adjusts synaptic weights based on the precise timing relationship between pre-synaptic and post-synaptic spikes. In its basic form, if a presynaptic neuron fires shortly before a postsynaptic neuron, the synapse is strengthened (long-term potentiation), whereas if it fires shortly after, the synapse is weakened (long-term depression) \cite{yamazakiSpikingNeuralNetworks2022}. Thus, STDP reinforces causal spike relationships (inspired by Hebbian plasticiy in the brain: "neurons that fire together wire together" \cite{hebb_organization_1949}) and encodes temporal correlations in the network. Variants of STDP have been explored to train SNNs without requiring labels, such as in \cite{paredes-vallesUnsupervisedLearningHierarchical2020}, where it was used for optical flow estimation.


\paragraph{Reinforcement Learning and Reward-Modulated Plasticity:} A third traning regime treats the SNN as an agent to be optimized via reward signals. In this approach, the SNN is trained with mehods akin to reinforcement learning (RL), where a scalar reward or error signal is used to modulate synaptic weights. STDP can be adapted to incorporate reward signals. For example, if a given synapse contributed to a correct decision or reward, its plasticity is adjusted to strengthen that connection (and vice verca for negative reward) \cite{yamazakiSpikingNeuralNetworks2022}. Recent works have shown SNNs can be used in depp RL tasks - in \cite{tangReinforcementCoLearningDeep2020}, spiking version of Deep Deterministic Policy Gradient (DDPG) was used for navigation tasks, \cite{zanattaExploringSpikingNeural2024a} showed spiking version of Proximal Policy Optimization (PPO) for solving continous control tasks in simulation
and, and in \cite{parkDesigningSpikingNeural2025}, a spiking verion of Twin Delayed Deep Deterministic Policy Gradien (TD3) was used for 3D robotics arm control. Overall, reinforcement learning in SNNs is less mature than in ANN domain, but it is a promising avenue, especially for decision-making tasks in robotics where reward signals are naturally available.

\section{Methodology}
Event-based cameras are a relatively new technology, and their integration with SNNs for control tasks is still an emerging field. This research will be conducted in a series of stages.

\subsection{Simulation Setup and Data Generation}
A key aspect of this research is development in a simulation environment that can generate event-based vision data while simulating the dynamics of quadrotors in a 3D environment. The simulation will be based on existing frameworks that simulate event from frame-based cameras, such as Vid2E \cite{gehrigVideoEventsRecycling2019}, V2CE \cite{ zhangV2CEVideoContinuous2023}, and accompanied by a physical simulator that simulates the dynamics of quadrotors in a 3D environment, such as Flightmare \cite{songFlightmareFlexibleQuadrotor2020}, OmniDrones \cite{xuOmniDronesEfficientFlexible2023}. The complete simulation environment will be used for gathering data, training and testing the complete end-to-end control framework. 

\subsection{End-to-End Control Framework}
The end-to-end control framework will be designed to process the event stream from the event camera and output control commands for the quadrotor. The framework will consist of several components:

\subsubsection{Event Data Representation}
As mentioned in Section \ref{sec:event_generation_model}, two schools of approaches can be distinguished for processing event data: event-by-event and batch methods (also known as contrast maximization). The method of choice affects the spatio-temporal resolution of the event stream output, robustness and processing complexity. The goal is to find a balance between low latency and robustness, while ensuring that the representation is suitable for agile flight and SNNs. 

\subsubsection{Control Policy}
Using the event data representation, the next step is to design a learning framework that can process the event stream and output control commands for the quadrotor. As mentioned in \cite{bhattacharyaMonocularEventBasedVision2024, loquercioLearningHighSpeedFlight2021}, depth is a more robust modality for autonomous flight, providing essential obstacle information while discarding unnecessary texture details. Hence, particular emphasis will be placed on representing the environment's depth using event data and ensuring its effective sim-to-real transfer. This depth representation is learned, and will serve as input to subsequent components of the control framework. Next, the depth representation will be processed by a spiking neural network to output control commands for the quadrotor. Two main approaches will be explored:

\paragraph{Teacher-Student Framework:} Following the teacher-student paradigm \cite{loquercioLearningHighSpeedFlight2021, bhattacharyaMonocularEventBasedVision2024}, a model-based teacher with privileged information about the environment will be distilled into a student SNN to produce control commands. The teacher, which holds knowledge of the full state of the environment and the quadrotor, generates safe trajectories. In this research, the student will use SNNs to learn safe trajectories from the teacher's control commands, given access only the desired flight direction, depth representation, ego-motion velocity and attitude.
    
\paragraph{Reinforcement Learning:} In this approach, the SNN will be trained using reinforcement learning (RL) to learn a control policy directly from the event stream and intertial measurements. Due to their inherent ability to capture input dynamics, Deep reinforcement learning (DRL), combined with SNN offers a compelling solution for tasks charcterized by temporal complexity. To this end, the SNN will be trained to produce control commands that maximize a reward signal that reflects the success of the quadrotor in navigating through the environment. An emphasis will be placed on learning a robust control policy that can handle unseen environments, and improve energy consumption. 

\subsubsection{Sim-to-Real Transfer}
The final step in the end-to-end control framework is to transfer the learned control policy from simulation to real-world quadrotors. Simulation can approximate the real-world dynamics up to a certain extent, but there are always discrepancies between the simulated and real-world environments. This is known as the sim-to-real gap, and it is a significant challenge in robotics. To address this, several techniques can be used: 
\paragraph{Domain Randomization:} This technique involves randomizing the simulation parameters, such as the quadrotor's dynamics, sensor noise, and environmental conditions, during training. This helps the SNN to learn a robust control policy that can generalize to real-world conditions \cite{loquercioDeepDroneRacing2020}.
\paragraph{Few-Shot Transfer} After training the SNN in simulation, it will be fine-tuned in the real world using a small amount of real-world data. This will help to adapt the learned control policy to the real-world dynamics and improve its performance \cite{bhattacharyaMonocularEventBasedVision2024}.

\subsection{Swarm Behavior}
Building upon the end-to-end control framework developed for single-agent quadrotors, the next step involves extending this framework to a swarm of agents. The swarm will be designed to exhibit emergent behavior, where the collective actions of the agents lead to complex behaviors without centralized control. Each individual agent will be controlled  by a local SNN-based policy, and adpat its strategy based on the behavior of neighboring agents and visual event stream input. Training will be first performed in simulation environments extended to support multi agent environments. As a first stage, the swarm will be trained to avoid intra-swarm collisions while avoiding dynamic obstacles in a controlled setting and static formation. Then, the swarm will be trained to perform more complex tasks, such as collective obstacle avoidance in cluttered environments the wild.  

\subsection{Neuromorphic Hardware and Benchmarking}
To fully leverage the advantages of SNNs and event cameras, the end-to-end control framework and swarm behavior will be implemented on neuromorphic hardware. Neuromorphic chips are still in their infancy, but they are designed to efficiently process spiking neural networks and event-based data streams. These chips, such as Intel's Loihi or IBM's TrueNorth, are specifically designed to exploit sparse spiking activity and offer significant advantages in terms of energy efficiency and real-time processing capabilities. The implementation will focus on optimizing the SNN architecture for neuromorphic hardware, ensuring that the control policies can run efficiently in real-time while consuming minimal power. Finally, the performance of the control frameworks will be benchmarked against traditional frame-based systems and other state-of-the-art methods. The evaluation will focus energy consumption, latency, and robustness.

\section{Novelty and Impact}
\paragraph{Novelty:} This research aims to address latency and energy challenges in autonomous high-speed flight by leveraging the unique capabilities of event cameras and spiking neural networks. While initial studies have demonstrated proof-of-concept SNN controlles for simple maneuvers or perception modules, fully end-to-end control pipelines that map asynchronous event streams to control commands remain scarce. Even more novel is the extension of this paradigm to multi-agent decentralized systems. By training SNNs directly from event streams to closed-loop control - and scaling this to swarms where each agent learns independently - this research introduces a new framework for efficient bio-inspired swarm autonomy. This approach is expected to advance energy efficiency and robustness of single and multi-agent robotic systems operating under power constraints, in high-speed and cluttered environments.

\paragraph{Impact:} The impact of this research is expected to be significant in several areas:
\begin{itemize}
    \item \textbf{Advancing High-Speed Autonomy:} By developing an end-to-end control framework that utilizes event cameras and SNNs, this research aims to improve latency and robusteness of robotic systems, enabling them to navigate and operate effectively in high-speed and cluttered environments.

    \item \textbf{Energy Efficiency:} By leveraging the energy-efficient nature of event cameras and SNNs, this research aims to significantly reduce the energy consumption of autonomous systems, making them more suitable for long-duration missions and battery-powered applications.    
    
    \item \textbf{Enhancing Swarm Intelligence:} The research will explore how local SNN-based policies can lead to emergent swarm behavior, enabling multi-agent systems to adapt to dynamic environments and exhibit complex behaviors without centralized control. 

\end{itemize}

\printbibliography[heading=bibnumbered]

asdasdasdasdasd

\end{document}